{"spans": [{"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "hE85qkP7Sac=", "parent_span_id": null, "name": "invoke_agent Strands Agents", "start_time_unix_nano": 1763272296311833300, "end_time_unix_nano": 1763272344795769200, "events": [{"name": "gen_ai.user.message", "time_unix_nano": 1763272297347058900, "attributes": {"content": "[{\"text\": \"Anthropic\"}]"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272344795769200, "attributes": {"message": "```json\n{\n  \"status\": \"success\",\n  \"company_or_url\": \"Anthropic\",\n  \"resolved_domain\": \"anthropic.com\",\n  \"sources\": [\n    {\n      \"url\": \"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use\",\n      \"policy_type\": \"cookie_policy\",\n      \"title\": \"What Cookies Does Anthropic Use? | Anthropic Help Center\",\n      \"summary\": \"Anthropic uses necessary cookies for basic service functionality (authentication, preferences, security), analytics cookies to enhance performance and understand user interaction, and marketing cookies for targeted advertising. Users can control cookie settings via 'Privacy Choices' on their website footer, 'Your privacy choices' in the Claude.ai menu, or by enabling global privacy controls in their browser.\",\n      \"relevance\": 0.97,\n      \"relevance_explanation\": \"Directly addresses the cookie policy and provides detailed information on types of cookies and user controls.\"\n    },\n    {\n      \"url\": \"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users\",\n      \"policy_type\": \"data_protection\",\n      \"title\": \"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\",\n      \"summary\": \"Anthropic protects user data through encryption (in transit and at rest) and limited employee access, with strict controls for Trust & Safety reviews. They implement industry-standard security measures including regular monitoring, anti-malware, multi-factor authentication, and annual employee training. For international data transfers, Anthropic ensures protection via adequacy decisions or Standard Contractual Clauses (SCCs).\",\n      \"relevance\": 0.92,\n      \"relevance_explanation\": \"Provides a comprehensive overview of data protection measures for Claude.ai users, including security, access, and international transfers.\"\n    },\n    {\n      \"url\": \"https://www.anthropic.com/news/updates-to-our-consumer-terms\",\n      \"policy_type\": \"privacy_policy\",\n      \"title\": \"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\",\n      \"summary\": \"Anthropic updated its Consumer Terms and Privacy Policy, allowing user data from Free, Pro, and Max plans (including Claude Code) to be used for model training, with user consent. Data retention is extended to five years if users opt-in for model training; otherwise, it remains 30 days. Anthropic uses tools to filter sensitive data and does not sell user data to third parties. Users can manage their preferences in Privacy Settings.\",\n      \"relevance\": 0.79,\n      \"relevance_explanation\": \"Details significant updates to privacy policy, specifically regarding data usage for model training, retention periods, and user control.\"\n    },\n    {\n      \"url\": \"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training\",\n      \"policy_type\": \"data_protection\",\n      \"title\": \"How do you use personal data in model training? | Anthropic Help Center\",\n      \"summary\": \"Anthropic trains its large language models using publicly available information, commercially licensed datasets, and user-provided data (with consent). While personal data may be incidentally included, it is not actively sought for training and is used solely to improve model effectiveness and safety, not for profiling or marketing. Privacy safeguards include de-linking user IDs from chats, filtering sensitive data, and post-training techniques to minimize personal data in outputs. Users retain control over their privacy settings.\",\n      \"relevance\": 0.79,\n      \"relevance_explanation\": \"Explains the sources of data for model training, how personal data is handled during training, and the privacy safeguards in place.\"\n    },\n    {\n      \"url\": \"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data\",\n      \"policy_type\": \"data_protection\",\n      \"title\": \"How long do you store my data? | Anthropic Privacy Center\",\n      \"summary\": \"Anthropic retains user data for up to 5 years if users consent to its use for model improvement, applying only to new or resumed chats. If consent is not given, chats are retained for 30 days. Users can delete conversations, which are removed from history immediately and from backend systems within 30 days. Data flagged for Usage Policy violations may be retained for up to 2 years (inputs/outputs) or 7 years (classification scores). Feedback data is retained for 5 years, and anonymized data may be kept longer for research.\",\n      \"relevance\": 0.78,\n      \"relevance_explanation\": \"Provides clear details on data retention periods based on user consent for model training and other scenarios like deletions and policy violations.\"\n    },\n    {\n      \"url\": \"https://www.anthropic.com/research/clio\",\n      \"policy_type\": \"other\",\n      \"title\": \"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\",\n      \"summary\": \"Anthropic developed 'Clio,' an automated analysis tool for privacy-preserving insights into real-world AI usage. Clio distills conversations into anonymized and aggregated topic clusters, allowing Anthropic to understand how models are used without exposing private user data. This system helps improve safety measures by identifying patterns of misuse and enhancing monitoring for high-stakes events, while rigorously maintaining user privacy through multi-layered safeguards and data minimization.\",\n      \"relevance\": 0.74,\n      \"relevance_explanation\": \"Describes a specific system (Clio) designed for privacy-preserving data analysis, offering insights into how Anthropic approaches data usage for safety and improvement while protecting user privacy.\"\n    },\n    {\n      \"url\": \"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta\",\n      \"policy_type\": \"data_protection\",\n      \"title\": \"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\",\n      \"summary\": \"For commercial customers using the Anthropic API, the 'Computer use' beta feature processes and collects screenshots from the computer's display, along with user inputs and outputs, to enable Claude to interpret and interact with interfaces. Anthropic does not collect additional data from the computer interface. Screenshots are automatically deleted from the backend within 30 days, unless otherwise agreed, and are retained to enforce the Usage Policy.\",\n      \"relevance\": 0.74,\n      \"relevance_explanation\": \"Details the specific personal data collected and retained for a particular beta feature ('Computer use') and its purpose.\"\n    },\n    {\n      \"url\": \"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates\",\n      \"policy_type\": \"terms_of_service\",\n      \"title\": \"Terms of Service Updates | Anthropic Help Center\",\n      \"summary\": \"Anthropic updated its Terms of Service to clarify the definition of 'Providers' to include affiliates, licensors, distributors, and service providers, making them intended third-party beneficiaries of disclaimers and liability limitations. The updates also clarify terms regarding software updates, including automatic updates to ensure users have access to the latest versions of their software.\",\n      \"relevance\": 0.72,\n      \"relevance_explanation\": \"Provides specific updates to the Terms of Service, focusing on definitions and software updates.\"\n    },\n    {\n      \"url\": \"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues\",\n      \"policy_type\": \"data_protection\",\n      \"title\": \"What is your approach to GDPR or related issues? | Anthropic Privacy Center\",\n      \"summary\": \"Anthropic adopts a holistic approach to data privacy and security, evaluating worldwide privacy laws and regulations, including GDPR, in the context of AI and large language models. They assess these regulations alongside customer needs to ensure comprehensive data protection practices.\",\n      \"relevance\": 0.72,\n      \"relevance_explanation\": \"Explains Anthropic's general approach to GDPR and global privacy regulations.\"\n    },\n    {\n      \"url\": \"https://www.anthropic.com/news/updating-our-usage-policy\",\n      \"policy_type\": \"terms_of_service\",\n      \"title\": \"Updating our Usage Policy \\\\\\\\ Anthropic\",\n      \"summary\": \"Anthropic updated its Acceptable Use Policy, now called 'Usage Policy,' to clarify allowed and prohibited applications of its products. Key changes include streamlining policies into 'Universal Usage Standards,' clarifying rules on election integrity and misinformation, adding requirements for high-risk use cases (e.g., healthcare, legal guidance), expanding law enforcement use cases, and enhancing privacy protections by explicitly forbidding biometric data analysis for inferring characteristics or content censorship on behalf of governments.\",\n      \"relevance\": 0.71,\n      \"relevance_explanation\": \"Details updates to their Usage Policy, which outlines acceptable use of their products and includes important privacy and data-related stipulations.\"\n    }\n  ],\n  \"error_message\": null\n}\n```\n", "finish_reason": "end_turn"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:51:37.347058+00:00", "gen_ai.operation.name": "invoke_agent", "gen_ai.system": "strands-agents", "gen_ai.agent.name": "Strands Agents", "gen_ai.request.model": "gemini-2.5-flash", "gen_ai.agent.tools": "[\"valyu_search\"]", "system_prompt": "\nYou are the SEARCH AGENT.\n\nInput: a company name (e.g. \"Anthropic\") OR a URL\n(e.g. \"https://www.anthropic.com/\").\n\nYou have access to a web search tool called `valyu_search`.\nUse it to find pages related to the company's data and privacy practices.\n\nYour goals:\n1. Resolve what company / site is being referred to.\n2. Use `valyu_search_tool` with smart queries to find:\n   - Privacy policy\n   - Terms of service / terms of use\n   - Cookie policy\n   - Data protection / data usage pages\n   - Any other clearly relevant pages about data collection / usage.\n3. For each relevant result, infer:\n   - policy_type:\n       \"privacy_policy\" | \"terms_of_service\" | \"cookie_policy\"\n       | \"data_protection\" | \"other\"\n   - url\n   - title\n   - a short summary (2-4 sentences) focused ONLY on:\n       - what data is collected,\n       - how it is used,\n       - how it is shared,\n       - retention / storage,\n       - user rights or controls.\n   - relevance: a float between 0 and 1\n4. Prefer official pages on the company's own domain.\n5. Avoid duplicates (same URL more than once).\n6. If no relevant sources are found, explain why.\n7. Try to limit sources to 5-10 high-quality results, if some repeat the same information, pick the best ones to keep.\n\nYou MUST ALWAYS return ONLY valid JSON, with this exact schema:\n\n{\n  \"status\": \"success\" | \"error\",\n  \"company_or_url\": \"<original input>\",\n  \"resolved_domain\": \"<domain-or-null>\",\n  \"sources\": [\n    {\n      \"url\": \"<string>\",\n      \"policy_type\": \"privacy_policy\" | \"terms_of_service\"\n                     | \"cookie_policy\" | \"data_protection\" | \"other\",\n      \"title\": \"<string or null>\",\n      \"summary\": \"<string or null>\",\n      \"relevance\": <float between 0 and 1>,\n      \"relevance_explanation\": \"<brief explanation of relevance score>\"\n    }\n  ],\n  \"error_message\": \"<string or null>\"\n}\n\nRules:\n- No markdown, no commentary, no code fences.\n- If something fails, set \"status\": \"error\" and explain in \"error_message\".\n- Sort sources by relevance descending.\n- ONLY use sources from the official company domain if possible, no third-party sites like Reddit.\n", "gen_ai.event.end_time": "2025-11-16T05:52:24.795769+00:00", "gen_ai.usage.prompt_tokens": 98691, "gen_ai.usage.completion_tokens": 7045, "gen_ai.usage.input_tokens": 98691, "gen_ai.usage.output_tokens": 7045, "gen_ai.usage.total_tokens": 105736, "gen_ai.usage.cache_read_input_tokens": 0, "gen_ai.usage.cache_write_input_tokens": 0, "mlflow.spanType": "\"AGENT\"", "mlflow.spanInputs": "[{\"role\": \"user\", \"content\": [{\"text\": \"Anthropic\"}]}]", "mlflow.spanOutputs": "\"```json\\n{\\n  \\\"status\\\": \\\"success\\\",\\n  \\\"company_or_url\\\": \\\"Anthropic\\\",\\n  \\\"resolved_domain\\\": \\\"anthropic.com\\\",\\n  \\\"sources\\\": [\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use\\\",\\n      \\\"policy_type\\\": \\\"cookie_policy\\\",\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Help Center\\\",\\n      \\\"summary\\\": \\\"Anthropic uses necessary cookies for basic service functionality (authentication, preferences, security), analytics cookies to enhance performance and understand user interaction, and marketing cookies for targeted advertising. Users can control cookie settings via 'Privacy Choices' on their website footer, 'Your privacy choices' in the Claude.ai menu, or by enabling global privacy controls in their browser.\\\",\\n      \\\"relevance\\\": 0.97,\\n      \\\"relevance_explanation\\\": \\\"Directly addresses the cookie policy and provides detailed information on types of cookies and user controls.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"Anthropic protects user data through encryption (in transit and at rest) and limited employee access, with strict controls for Trust & Safety reviews. They implement industry-standard security measures including regular monitoring, anti-malware, multi-factor authentication, and annual employee training. For international data transfers, Anthropic ensures protection via adequacy decisions or Standard Contractual Clauses (SCCs).\\\",\\n      \\\"relevance\\\": 0.92,\\n      \\\"relevance_explanation\\\": \\\"Provides a comprehensive overview of data protection measures for Claude.ai users, including security, access, and international transfers.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms\\\",\\n      \\\"policy_type\\\": \\\"privacy_policy\\\",\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\\\\\\\\\ Anthropic\\\",\\n      \\\"summary\\\": \\\"Anthropic updated its Consumer Terms and Privacy Policy, allowing user data from Free, Pro, and Max plans (including Claude Code) to be used for model training, with user consent. Data retention is extended to five years if users opt-in for model training; otherwise, it remains 30 days. Anthropic uses tools to filter sensitive data and does not sell user data to third parties. Users can manage their preferences in Privacy Settings.\\\",\\n      \\\"relevance\\\": 0.79,\\n      \\\"relevance_explanation\\\": \\\"Details significant updates to privacy policy, specifically regarding data usage for model training, retention periods, and user control.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"summary\\\": \\\"Anthropic trains its large language models using publicly available information, commercially licensed datasets, and user-provided data (with consent). While personal data may be incidentally included, it is not actively sought for training and is used solely to improve model effectiveness and safety, not for profiling or marketing. Privacy safeguards include de-linking user IDs from chats, filtering sensitive data, and post-training techniques to minimize personal data in outputs. Users retain control over their privacy settings.\\\",\\n      \\\"relevance\\\": 0.79,\\n      \\\"relevance_explanation\\\": \\\"Explains the sources of data for model training, how personal data is handled during training, and the privacy safeguards in place.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"Anthropic retains user data for up to 5 years if users consent to its use for model improvement, applying only to new or resumed chats. If consent is not given, chats are retained for 30 days. Users can delete conversations, which are removed from history immediately and from backend systems within 30 days. Data flagged for Usage Policy violations may be retained for up to 2 years (inputs/outputs) or 7 years (classification scores). Feedback data is retained for 5 years, and anonymized data may be kept longer for research.\\\",\\n      \\\"relevance\\\": 0.78,\\n      \\\"relevance_explanation\\\": \\\"Provides clear details on data retention periods based on user consent for model training and other scenarios like deletions and policy violations.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio\\\",\\n      \\\"policy_type\\\": \\\"other\\\",\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\\\\\\\\\ Anthropic\\\",\\n      \\\"summary\\\": \\\"Anthropic developed 'Clio,' an automated analysis tool for privacy-preserving insights into real-world AI usage. Clio distills conversations into anonymized and aggregated topic clusters, allowing Anthropic to understand how models are used without exposing private user data. This system helps improve safety measures by identifying patterns of misuse and enhancing monitoring for high-stakes events, while rigorously maintaining user privacy through multi-layered safeguards and data minimization.\\\",\\n      \\\"relevance\\\": 0.74,\\n      \\\"relevance_explanation\\\": \\\"Describes a specific system (Clio) designed for privacy-preserving data analysis, offering insights into how Anthropic approaches data usage for safety and improvement while protecting user privacy.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"For commercial customers using the Anthropic API, the 'Computer use' beta feature processes and collects screenshots from the computer's display, along with user inputs and outputs, to enable Claude to interpret and interact with interfaces. Anthropic does not collect additional data from the computer interface. Screenshots are automatically deleted from the backend within 30 days, unless otherwise agreed, and are retained to enforce the Usage Policy.\\\",\\n      \\\"relevance\\\": 0.74,\\n      \\\"relevance_explanation\\\": \\\"Details the specific personal data collected and retained for a particular beta feature ('Computer use') and its purpose.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates\\\",\\n      \\\"policy_type\\\": \\\"terms_of_service\\\",\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"summary\\\": \\\"Anthropic updated its Terms of Service to clarify the definition of 'Providers' to include affiliates, licensors, distributors, and service providers, making them intended third-party beneficiaries of disclaimers and liability limitations. The updates also clarify terms regarding software updates, including automatic updates to ensure users have access to the latest versions of their software.\\\",\\n      \\\"relevance\\\": 0.72,\\n      \\\"relevance_explanation\\\": \\\"Provides specific updates to the Terms of Service, focusing on definitions and software updates.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"Anthropic adopts a holistic approach to data privacy and security, evaluating worldwide privacy laws and regulations, including GDPR, in the context of AI and large language models. They assess these regulations alongside customer needs to ensure comprehensive data protection practices.\\\",\\n      \\\"relevance\\\": 0.72,\\n      \\\"relevance_explanation\\\": \\\"Explains Anthropic's general approach to GDPR and global privacy regulations.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy\\\",\\n      \\\"policy_type\\\": \\\"terms_of_service\\\",\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\\\\\\\\\ Anthropic\\\",\\n      \\\"summary\\\": \\\"Anthropic updated its Acceptable Use Policy, now called 'Usage Policy,' to clarify allowed and prohibited applications of its products. Key changes include streamlining policies into 'Universal Usage Standards,' clarifying rules on election integrity and misinformation, adding requirements for high-risk use cases (e.g., healthcare, legal guidance), expanding law enforcement use cases, and enhancing privacy protections by explicitly forbidding biometric data analysis for inferring characteristics or content censorship on behalf of governments.\\\",\\n      \\\"relevance\\\": 0.71,\\n      \\\"relevance_explanation\\\": \\\"Details updates to their Usage Policy, which outlines acceptable use of their products and includes important privacy and data-related stipulations.\\\"\\n    }\\n  ],\\n  \\\"error_message\\\": null\\n}\\n```\\n\""}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "p2svX2wECSg=", "parent_span_id": "hE85qkP7Sac=", "name": "execute_event_loop_cycle", "start_time_unix_nano": 1763272297347058900, "end_time_unix_nano": 1763272301482396900, "events": [{"name": "gen_ai.user.message", "time_unix_nano": 1763272297348064600, "attributes": {"content": "[{\"text\": \"Anthropic\"}]"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272301482396900, "attributes": {"message": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"Anthropic official website\"}}}]", "tool.result": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "mlflow.spanType": "null", "gen_ai.event.start_time": "2025-11-16T05:51:37.348064+00:00", "event_loop.cycle_id": "6c2bcade-9ef5-46ae-8734-0f9015c5366a", "gen_ai.event.end_time": "2025-11-16T05:51:41.482396+00:00", "mlflow.spanInputs": "[{\"role\": \"user\", \"content\": [{\"text\": \"Anthropic\"}]}]", "mlflow.spanOutputs": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"Anthropic official website\"}}}]"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "P42nn6xjt4w=", "parent_span_id": "p2svX2wECSg=", "name": "chat", "start_time_unix_nano": 1763272297348064600, "end_time_unix_nano": 1763272300308669800, "events": [{"name": "gen_ai.user.message", "time_unix_nano": 1763272297348064600, "attributes": {"content": "[{\"text\": \"Anthropic\"}]"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272300308669800, "attributes": {"finish_reason": "tool_use", "message": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"Anthropic official website\"}}}]"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:51:37.348064+00:00", "gen_ai.operation.name": "chat", "gen_ai.system": "strands-agents", "gen_ai.request.model": "gemini-2.5-flash", "gen_ai.event.end_time": "2025-11-16T05:51:40.308669+00:00", "gen_ai.usage.prompt_tokens": 660, "gen_ai.usage.input_tokens": 660, "gen_ai.usage.completion_tokens": 150, "gen_ai.usage.output_tokens": 150, "gen_ai.usage.total_tokens": 810, "gen_ai.server.time_to_first_token": 2957, "mlflow.spanType": "\"CHAT_MODEL\"", "mlflow.spanInputs": "[{\"role\": \"user\", \"content\": [{\"text\": \"Anthropic\"}]}]", "mlflow.spanOutputs": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"Anthropic official website\"}}}]", "mlflow.chat.tokenUsage": "{\"input_tokens\": 660, \"output_tokens\": 150, \"total_tokens\": 810}"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "AgHkyKPbYRQ=", "parent_span_id": "p2svX2wECSg=", "name": "execute_tool valyu_search", "start_time_unix_nano": 1763272300317675000, "end_time_unix_nano": 1763272301481397300, "events": [{"name": "gen_ai.tool.message", "time_unix_nano": 1763272300317675000, "attributes": {"role": "tool", "content": "{\"prompt\": \"Anthropic official website\"}", "id": "valyu_search"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272301481397300, "attributes": {"message": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]", "id": "valyu_search"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:51:40.317675+00:00", "gen_ai.operation.name": "execute_tool", "gen_ai.system": "strands-agents", "gen_ai.tool.name": "valyu_search", "gen_ai.tool.call.id": "valyu_search", "gen_ai.tool.description": "Returns Valyu results from a prompt\n\nReturns:\n    SearchResponse: the response with all the results", "gen_ai.tool.json_schema": "{\"properties\": {\"prompt\": {\"description\": \"the prompt to search for\", \"type\": \"string\"}}, \"required\": [\"prompt\"], \"type\": \"object\"}", "gen_ai.event.end_time": "2025-11-16T05:51:41.481397+00:00", "gen_ai.tool.status": "success", "mlflow.spanType": "\"TOOL\"", "mlflow.spanInputs": "[{\"role\": \"tool\", \"content\": {\"prompt\": \"Anthropic official website\"}}]", "mlflow.spanOutputs": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "JBDlr/T1z4c=", "parent_span_id": "hE85qkP7Sac=", "name": "execute_event_loop_cycle", "start_time_unix_nano": 1763272301483399100, "end_time_unix_nano": 1763272308823883400, "events": [{"name": "gen_ai.user.message", "time_unix_nano": 1763272301483399100, "attributes": {"content": "[{\"text\": \"Anthropic\"}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272301483399100, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"Anthropic official website\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272301483399100, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272308823883400, "attributes": {"message": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com privacy policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com terms of service\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com cookie policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com data protection\"}}}]", "tool.result": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]}}]"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "mlflow.spanType": "null", "gen_ai.event.start_time": "2025-11-16T05:51:41.483399+00:00", "event_loop.cycle_id": "41fe421a-0cac-4be6-a8c0-042af5124778", "event_loop.parent_cycle_id": "6c2bcade-9ef5-46ae-8734-0f9015c5366a", "gen_ai.event.end_time": "2025-11-16T05:51:48.823883+00:00", "mlflow.spanInputs": "[{\"role\": \"user\", \"content\": [{\"text\": \"Anthropic\"}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]}]", "mlflow.spanOutputs": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com privacy policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com terms of service\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com cookie policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com data protection\"}}}]"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "62MJObC7f4I=", "parent_span_id": "JBDlr/T1z4c=", "name": "chat", "start_time_unix_nano": 1763272301483399100, "end_time_unix_nano": 1763272304587585300, "events": [{"name": "gen_ai.user.message", "time_unix_nano": 1763272301484397800, "attributes": {"content": "[{\"text\": \"Anthropic\"}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272301484397800, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"Anthropic official website\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272301484397800, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272304587585300, "attributes": {"finish_reason": "tool_use", "message": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com privacy policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com terms of service\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com cookie policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com data protection\"}}}]"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:51:41.484397+00:00", "gen_ai.operation.name": "chat", "gen_ai.system": "strands-agents", "gen_ai.request.model": "gemini-2.5-flash", "gen_ai.event.end_time": "2025-11-16T05:51:44.587585+00:00", "gen_ai.usage.prompt_tokens": 14623, "gen_ai.usage.input_tokens": 14623, "gen_ai.usage.completion_tokens": 172, "gen_ai.usage.output_tokens": 172, "gen_ai.usage.total_tokens": 14795, "gen_ai.server.time_to_first_token": 3100, "mlflow.spanType": "\"CHAT_MODEL\"", "mlflow.spanInputs": "[{\"role\": \"user\", \"content\": [{\"text\": \"Anthropic\"}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]}]", "mlflow.spanOutputs": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com privacy policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com terms of service\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com cookie policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com data protection\"}}}]", "mlflow.chat.tokenUsage": "{\"input_tokens\": 14623, \"output_tokens\": 172, \"total_tokens\": 14795}"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "Iwfxo+mkj9g=", "parent_span_id": "JBDlr/T1z4c=", "name": "execute_tool valyu_search", "start_time_unix_nano": 1763272304588581700, "end_time_unix_nano": 1763272308821888600, "events": [{"name": "gen_ai.tool.message", "time_unix_nano": 1763272304588581700, "attributes": {"role": "tool", "content": "{\"prompt\": \"site:anthropic.com privacy policy\"}", "id": "valyu_search"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272308821888600, "attributes": {"message": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]", "id": "valyu_search"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:51:44.588581+00:00", "gen_ai.operation.name": "execute_tool", "gen_ai.system": "strands-agents", "gen_ai.tool.name": "valyu_search", "gen_ai.tool.call.id": "valyu_search", "gen_ai.tool.description": "Returns Valyu results from a prompt\n\nReturns:\n    SearchResponse: the response with all the results", "gen_ai.tool.json_schema": "{\"properties\": {\"prompt\": {\"description\": \"the prompt to search for\", \"type\": \"string\"}}, \"required\": [\"prompt\"], \"type\": \"object\"}", "gen_ai.event.end_time": "2025-11-16T05:51:48.821888+00:00", "gen_ai.tool.status": "success", "mlflow.spanType": "\"TOOL\"", "mlflow.spanInputs": "[{\"role\": \"tool\", \"content\": {\"prompt\": \"site:anthropic.com privacy policy\"}}]", "mlflow.spanOutputs": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "SRIVeEGWufI=", "parent_span_id": "JBDlr/T1z4c=", "name": "execute_tool valyu_search", "start_time_unix_nano": 1763272304588581700, "end_time_unix_nano": 1763272308343227700, "events": [{"name": "gen_ai.tool.message", "time_unix_nano": 1763272304589584000, "attributes": {"role": "tool", "content": "{\"prompt\": \"site:anthropic.com terms of service\"}", "id": "valyu_search"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272308343227700, "attributes": {"message": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]", "id": "valyu_search"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:51:44.589584+00:00", "gen_ai.operation.name": "execute_tool", "gen_ai.system": "strands-agents", "gen_ai.tool.name": "valyu_search", "gen_ai.tool.call.id": "valyu_search", "gen_ai.tool.description": "Returns Valyu results from a prompt\n\nReturns:\n    SearchResponse: the response with all the results", "gen_ai.tool.json_schema": "{\"properties\": {\"prompt\": {\"description\": \"the prompt to search for\", \"type\": \"string\"}}, \"required\": [\"prompt\"], \"type\": \"object\"}", "gen_ai.event.end_time": "2025-11-16T05:51:48.343227+00:00", "gen_ai.tool.status": "success", "mlflow.spanType": "\"TOOL\"", "mlflow.spanInputs": "[{\"role\": \"tool\", \"content\": {\"prompt\": \"site:anthropic.com terms of service\"}}]", "mlflow.spanOutputs": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "xEQdwVK/JyI=", "parent_span_id": "JBDlr/T1z4c=", "name": "execute_tool valyu_search", "start_time_unix_nano": 1763272304589584000, "end_time_unix_nano": 1763272308590418200, "events": [{"name": "gen_ai.tool.message", "time_unix_nano": 1763272304589584000, "attributes": {"role": "tool", "content": "{\"prompt\": \"site:anthropic.com cookie policy\"}", "id": "valyu_search"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272308590418200, "attributes": {"message": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]", "id": "valyu_search"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:51:44.589584+00:00", "gen_ai.operation.name": "execute_tool", "gen_ai.system": "strands-agents", "gen_ai.tool.name": "valyu_search", "gen_ai.tool.call.id": "valyu_search", "gen_ai.tool.description": "Returns Valyu results from a prompt\n\nReturns:\n    SearchResponse: the response with all the results", "gen_ai.tool.json_schema": "{\"properties\": {\"prompt\": {\"description\": \"the prompt to search for\", \"type\": \"string\"}}, \"required\": [\"prompt\"], \"type\": \"object\"}", "gen_ai.event.end_time": "2025-11-16T05:51:48.590418+00:00", "gen_ai.tool.status": "success", "mlflow.spanType": "\"TOOL\"", "mlflow.spanInputs": "[{\"role\": \"tool\", \"content\": {\"prompt\": \"site:anthropic.com cookie policy\"}}]", "mlflow.spanOutputs": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "4yOSGYcPIuo=", "parent_span_id": "JBDlr/T1z4c=", "name": "execute_tool valyu_search", "start_time_unix_nano": 1763272304591581200, "end_time_unix_nano": 1763272308410929400, "events": [{"name": "gen_ai.tool.message", "time_unix_nano": 1763272304592582700, "attributes": {"role": "tool", "content": "{\"prompt\": \"site:anthropic.com data protection\"}", "id": "valyu_search"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272308410929400, "attributes": {"message": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]", "id": "valyu_search"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:51:44.592582+00:00", "gen_ai.operation.name": "execute_tool", "gen_ai.system": "strands-agents", "gen_ai.tool.name": "valyu_search", "gen_ai.tool.call.id": "valyu_search", "gen_ai.tool.description": "Returns Valyu results from a prompt\n\nReturns:\n    SearchResponse: the response with all the results", "gen_ai.tool.json_schema": "{\"properties\": {\"prompt\": {\"description\": \"the prompt to search for\", \"type\": \"string\"}}, \"required\": [\"prompt\"], \"type\": \"object\"}", "gen_ai.event.end_time": "2025-11-16T05:51:48.410929+00:00", "gen_ai.tool.status": "success", "mlflow.spanType": "\"TOOL\"", "mlflow.spanInputs": "[{\"role\": \"tool\", \"content\": {\"prompt\": \"site:anthropic.com data protection\"}}]", "mlflow.spanOutputs": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "IXA0up7o2T0=", "parent_span_id": "hE85qkP7Sac=", "name": "execute_event_loop_cycle", "start_time_unix_nano": 1763272308823883400, "end_time_unix_nano": 1763272321131151000, "events": [{"name": "gen_ai.user.message", "time_unix_nano": 1763272308824883400, "attributes": {"content": "[{\"text\": \"Anthropic\"}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272308824883400, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"Anthropic official website\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272308824883400, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272308824883400, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com privacy policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com terms of service\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com cookie policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com data protection\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272308825881600, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]}}]"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272321131151000, "attributes": {"message": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com What Cookies Does Anthropic Use?\"}}}]", "tool.result": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_edc96ff4-ee89-49d8-9f9b-05b8f2a3ef70\\\",\\n  \\\"query\\\": \\\"site:anthropic.com What Cookies Does Anthropic Use?\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Analytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.97,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023541-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Necessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security. Analytics cookies are not necessary to deliver our Services, but they do enhance performance of ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.968687908314245,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Claude.ai &amp; Claude.ai Pro users · 22 articles · Cookies · What Cookies Does Anthropic Use? Data Handling &amp; Retention · How long do you store my data? How Do You Use Personal Data in Model Training? Is my data used for model training? What personal data is collected when using dictation on the Claude mobile apps?\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.798185850487796,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 9099\\n}\"}]}}]"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "mlflow.spanType": "null", "gen_ai.event.start_time": "2025-11-16T05:51:48.824883+00:00", "event_loop.cycle_id": "25bff767-8f3d-4ac2-af0d-99ef345cc503", "event_loop.parent_cycle_id": "41fe421a-0cac-4be6-a8c0-042af5124778", "gen_ai.event.end_time": "2025-11-16T05:52:01.131151+00:00", "mlflow.spanInputs": "[{\"role\": \"user\", \"content\": [{\"text\": \"Anthropic\"}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]}}]}]", "mlflow.spanOutputs": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com What Cookies Does Anthropic Use?\"}}}]"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "xZ9r4ag3FfQ=", "parent_span_id": "IXA0up7o2T0=", "name": "chat", "start_time_unix_nano": 1763272308825881600, "end_time_unix_nano": 1763272318181293800, "events": [{"name": "gen_ai.user.message", "time_unix_nano": 1763272308825881600, "attributes": {"content": "[{\"text\": \"Anthropic\"}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272308825881600, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"Anthropic official website\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272308826883500, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272308826883500, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com privacy policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com terms of service\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com cookie policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com data protection\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272308827881100, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]}}]"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272318181293800, "attributes": {"finish_reason": "tool_use", "message": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com What Cookies Does Anthropic Use?\"}}}]"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:51:48.825881+00:00", "gen_ai.operation.name": "chat", "gen_ai.system": "strands-agents", "gen_ai.request.model": "gemini-2.5-flash", "gen_ai.event.end_time": "2025-11-16T05:51:58.181293+00:00", "gen_ai.usage.prompt_tokens": 39338, "gen_ai.usage.input_tokens": 39338, "gen_ai.usage.completion_tokens": 1791, "gen_ai.usage.output_tokens": 1791, "gen_ai.usage.total_tokens": 41129, "gen_ai.server.time_to_first_token": 9351, "mlflow.spanType": "\"CHAT_MODEL\"", "mlflow.spanInputs": "[{\"role\": \"user\", \"content\": [{\"text\": \"Anthropic\"}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]}}]}]", "mlflow.spanOutputs": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com What Cookies Does Anthropic Use?\"}}}]", "mlflow.chat.tokenUsage": "{\"input_tokens\": 39338, \"output_tokens\": 1791, \"total_tokens\": 41129}"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "2jh+/FgMz9U=", "parent_span_id": "IXA0up7o2T0=", "name": "execute_tool valyu_search", "start_time_unix_nano": 1763272318183293400, "end_time_unix_nano": 1763272321130070900, "events": [{"name": "gen_ai.tool.message", "time_unix_nano": 1763272318183293400, "attributes": {"role": "tool", "content": "{\"prompt\": \"site:anthropic.com What Cookies Does Anthropic Use?\"}", "id": "valyu_search"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272321130070900, "attributes": {"message": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_edc96ff4-ee89-49d8-9f9b-05b8f2a3ef70\\\",\\n  \\\"query\\\": \\\"site:anthropic.com What Cookies Does Anthropic Use?\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Analytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.97,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023541-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Necessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security. Analytics cookies are not necessary to deliver our Services, but they do enhance performance of ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.968687908314245,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Claude.ai &amp; Claude.ai Pro users · 22 articles · Cookies · What Cookies Does Anthropic Use? Data Handling &amp; Retention · How long do you store my data? How Do You Use Personal Data in Model Training? Is my data used for model training? What personal data is collected when using dictation on the Claude mobile apps?\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.798185850487796,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 9099\\n}\"}]", "id": "valyu_search"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:51:58.183293+00:00", "gen_ai.operation.name": "execute_tool", "gen_ai.system": "strands-agents", "gen_ai.tool.name": "valyu_search", "gen_ai.tool.call.id": "valyu_search", "gen_ai.tool.description": "Returns Valyu results from a prompt\n\nReturns:\n    SearchResponse: the response with all the results", "gen_ai.tool.json_schema": "{\"properties\": {\"prompt\": {\"description\": \"the prompt to search for\", \"type\": \"string\"}}, \"required\": [\"prompt\"], \"type\": \"object\"}", "gen_ai.event.end_time": "2025-11-16T05:52:01.130070+00:00", "gen_ai.tool.status": "success", "mlflow.spanType": "\"TOOL\"", "mlflow.spanInputs": "[{\"role\": \"tool\", \"content\": {\"prompt\": \"site:anthropic.com What Cookies Does Anthropic Use?\"}}]", "mlflow.spanOutputs": "[{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_edc96ff4-ee89-49d8-9f9b-05b8f2a3ef70\\\",\\n  \\\"query\\\": \\\"site:anthropic.com What Cookies Does Anthropic Use?\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Analytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.97,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023541-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Necessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security. Analytics cookies are not necessary to deliver our Services, but they do enhance performance of ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.968687908314245,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Claude.ai &amp; Claude.ai Pro users · 22 articles · Cookies · What Cookies Does Anthropic Use? Data Handling &amp; Retention · How long do you store my data? How Do You Use Personal Data in Model Training? Is my data used for model training? What personal data is collected when using dictation on the Claude mobile apps?\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.798185850487796,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 9099\\n}\"}]"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "X28RVy3qmxo=", "parent_span_id": "hE85qkP7Sac=", "name": "execute_event_loop_cycle", "start_time_unix_nano": 1763272321132159500, "end_time_unix_nano": 1763272344793769400, "events": [{"name": "gen_ai.user.message", "time_unix_nano": 1763272321132159500, "attributes": {"content": "[{\"text\": \"Anthropic\"}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272321132159500, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"Anthropic official website\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272321133152800, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272321133152800, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com privacy policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com terms of service\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com cookie policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com data protection\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272321134153000, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]}}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272321134153000, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com What Cookies Does Anthropic Use?\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272321134153000, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_edc96ff4-ee89-49d8-9f9b-05b8f2a3ef70\\\",\\n  \\\"query\\\": \\\"site:anthropic.com What Cookies Does Anthropic Use?\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Analytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.97,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023541-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Necessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security. Analytics cookies are not necessary to deliver our Services, but they do enhance performance of ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.968687908314245,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Claude.ai &amp; Claude.ai Pro users · 22 articles · Cookies · What Cookies Does Anthropic Use? Data Handling &amp; Retention · How long do you store my data? How Do You Use Personal Data in Model Training? Is my data used for model training? What personal data is collected when using dictation on the Claude mobile apps?\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.798185850487796,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 9099\\n}\"}]}}]"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "mlflow.spanType": "null", "gen_ai.event.start_time": "2025-11-16T05:52:01.132159+00:00", "event_loop.cycle_id": "f189c121-f29b-4fe1-8402-7939cb139fde", "event_loop.parent_cycle_id": "25bff767-8f3d-4ac2-af0d-99ef345cc503", "gen_ai.event.end_time": "2025-11-16T05:52:24.793769+00:00", "mlflow.spanInputs": "[{\"role\": \"user\", \"content\": [{\"text\": \"Anthropic\"}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]}}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_edc96ff4-ee89-49d8-9f9b-05b8f2a3ef70\\\",\\n  \\\"query\\\": \\\"site:anthropic.com What Cookies Does Anthropic Use?\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Analytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.97,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023541-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Necessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security. Analytics cookies are not necessary to deliver our Services, but they do enhance performance of ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.968687908314245,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Claude.ai &amp; Claude.ai Pro users · 22 articles · Cookies · What Cookies Does Anthropic Use? Data Handling &amp; Retention · How long do you store my data? How Do You Use Personal Data in Model Training? Is my data used for model training? What personal data is collected when using dictation on the Claude mobile apps?\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.798185850487796,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 9099\\n}\"}]}}]}]"}}, {"trace_id": "4plngrBy5MyIvNxg1B+qsQ==", "span_id": "K/6l20tmfcU=", "parent_span_id": "X28RVy3qmxo=", "name": "chat", "start_time_unix_nano": 1763272321134153000, "end_time_unix_nano": 1763272344790771900, "events": [{"name": "gen_ai.user.message", "time_unix_nano": 1763272321134153000, "attributes": {"content": "[{\"text\": \"Anthropic\"}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272321134153000, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"Anthropic official website\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272321134153000, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272321134153000, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com privacy policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com terms of service\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com cookie policy\"}}}, {\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com data protection\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272321135184600, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]}}]"}}, {"name": "gen_ai.assistant.message", "time_unix_nano": 1763272321135184600, "attributes": {"content": "[{\"toolUse\": {\"toolUseId\": \"valyu_search\", \"name\": \"valyu_search\", \"input\": {\"prompt\": \"site:anthropic.com What Cookies Does Anthropic Use?\"}}}]"}}, {"name": "gen_ai.tool.message", "time_unix_nano": 1763272321136177500, "attributes": {"content": "[{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_edc96ff4-ee89-49d8-9f9b-05b8f2a3ef70\\\",\\n  \\\"query\\\": \\\"site:anthropic.com What Cookies Does Anthropic Use?\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Analytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.97,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023541-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Necessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security. Analytics cookies are not necessary to deliver our Services, but they do enhance performance of ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.968687908314245,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Claude.ai &amp; Claude.ai Pro users · 22 articles · Cookies · What Cookies Does Anthropic Use? Data Handling &amp; Retention · How long do you store my data? How Do You Use Personal Data in Model Training? Is my data used for model training? What personal data is collected when using dictation on the Claude mobile apps?\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.798185850487796,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 9099\\n}\"}]}}]"}}, {"name": "gen_ai.choice", "time_unix_nano": 1763272344790771900, "attributes": {"finish_reason": "end_turn", "message": "[{\"text\": \"```json\\n{\\n  \\\"status\\\": \\\"success\\\",\\n  \\\"company_or_url\\\": \\\"Anthropic\\\",\\n  \\\"resolved_domain\\\": \\\"anthropic.com\\\",\\n  \\\"sources\\\": [\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use\\\",\\n      \\\"policy_type\\\": \\\"cookie_policy\\\",\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Help Center\\\",\\n      \\\"summary\\\": \\\"Anthropic uses necessary cookies for basic service functionality (authentication, preferences, security), analytics cookies to enhance performance and understand user interaction, and marketing cookies for targeted advertising. Users can control cookie settings via 'Privacy Choices' on their website footer, 'Your privacy choices' in the Claude.ai menu, or by enabling global privacy controls in their browser.\\\",\\n      \\\"relevance\\\": 0.97,\\n      \\\"relevance_explanation\\\": \\\"Directly addresses the cookie policy and provides detailed information on types of cookies and user controls.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"Anthropic protects user data through encryption (in transit and at rest) and limited employee access, with strict controls for Trust & Safety reviews. They implement industry-standard security measures including regular monitoring, anti-malware, multi-factor authentication, and annual employee training. For international data transfers, Anthropic ensures protection via adequacy decisions or Standard Contractual Clauses (SCCs).\\\",\\n      \\\"relevance\\\": 0.92,\\n      \\\"relevance_explanation\\\": \\\"Provides a comprehensive overview of data protection measures for Claude.ai users, including security, access, and international transfers.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms\\\",\\n      \\\"policy_type\\\": \\\"privacy_policy\\\",\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\\\\\\\\\ Anthropic\\\",\\n      \\\"summary\\\": \\\"Anthropic updated its Consumer Terms and Privacy Policy, allowing user data from Free, Pro, and Max plans (including Claude Code) to be used for model training, with user consent. Data retention is extended to five years if users opt-in for model training; otherwise, it remains 30 days. Anthropic uses tools to filter sensitive data and does not sell user data to third parties. Users can manage their preferences in Privacy Settings.\\\",\\n      \\\"relevance\\\": 0.79,\\n      \\\"relevance_explanation\\\": \\\"Details significant updates to privacy policy, specifically regarding data usage for model training, retention periods, and user control.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"summary\\\": \\\"Anthropic trains its large language models using publicly available information, commercially licensed datasets, and user-provided data (with consent). While personal data may be incidentally included, it is not actively sought for training and is used solely to improve model effectiveness and safety, not for profiling or marketing. Privacy safeguards include de-linking user IDs from chats, filtering sensitive data, and post-training techniques to minimize personal data in outputs. Users retain control over their privacy settings.\\\",\\n      \\\"relevance\\\": 0.79,\\n      \\\"relevance_explanation\\\": \\\"Explains the sources of data for model training, how personal data is handled during training, and the privacy safeguards in place.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"Anthropic retains user data for up to 5 years if users consent to its use for model improvement, applying only to new or resumed chats. If consent is not given, chats are retained for 30 days. Users can delete conversations, which are removed from history immediately and from backend systems within 30 days. Data flagged for Usage Policy violations may be retained for up to 2 years (inputs/outputs) or 7 years (classification scores). Feedback data is retained for 5 years, and anonymized data may be kept longer for research.\\\",\\n      \\\"relevance\\\": 0.78,\\n      \\\"relevance_explanation\\\": \\\"Provides clear details on data retention periods based on user consent for model training and other scenarios like deletions and policy violations.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio\\\",\\n      \\\"policy_type\\\": \\\"other\\\",\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\\\\\\\\\ Anthropic\\\",\\n      \\\"summary\\\": \\\"Anthropic developed 'Clio,' an automated analysis tool for privacy-preserving insights into real-world AI usage. Clio distills conversations into anonymized and aggregated topic clusters, allowing Anthropic to understand how models are used without exposing private user data. This system helps improve safety measures by identifying patterns of misuse and enhancing monitoring for high-stakes events, while rigorously maintaining user privacy through multi-layered safeguards and data minimization.\\\",\\n      \\\"relevance\\\": 0.74,\\n      \\\"relevance_explanation\\\": \\\"Describes a specific system (Clio) designed for privacy-preserving data analysis, offering insights into how Anthropic approaches data usage for safety and improvement while protecting user privacy.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"For commercial customers using the Anthropic API, the 'Computer use' beta feature processes and collects screenshots from the computer's display, along with user inputs and outputs, to enable Claude to interpret and interact with interfaces. Anthropic does not collect additional data from the computer interface. Screenshots are automatically deleted from the backend within 30 days, unless otherwise agreed, and are retained to enforce the Usage Policy.\\\",\\n      \\\"relevance\\\": 0.74,\\n      \\\"relevance_explanation\\\": \\\"Details the specific personal data collected and retained for a particular beta feature ('Computer use') and its purpose.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates\\\",\\n      \\\"policy_type\\\": \\\"terms_of_service\\\",\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"summary\\\": \\\"Anthropic updated its Terms of Service to clarify the definition of 'Providers' to include affiliates, licensors, distributors, and service providers, making them intended third-party beneficiaries of disclaimers and liability limitations. The updates also clarify terms regarding software updates, including automatic updates to ensure users have access to the latest versions of their software.\\\",\\n      \\\"relevance\\\": 0.72,\\n      \\\"relevance_explanation\\\": \\\"Provides specific updates to the Terms of Service, focusing on definitions and software updates.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"Anthropic adopts a holistic approach to data privacy and security, evaluating worldwide privacy laws and regulations, including GDPR, in the context of AI and large language models. They assess these regulations alongside customer needs to ensure comprehensive data protection practices.\\\",\\n      \\\"relevance\\\": 0.72,\\n      \\\"relevance_explanation\\\": \\\"Explains Anthropic's general approach to GDPR and global privacy regulations.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy\\\",\\n      \\\"policy_type\\\": \\\"terms_of_service\\\",\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\\\\\\\\\ Anthropic\\\",\\n      \\\"summary\\\": \\\"Anthropic updated its Acceptable Use Policy, now called 'Usage Policy,' to clarify allowed and prohibited applications of its products. Key changes include streamlining policies into 'Universal Usage Standards,' clarifying rules on election integrity and misinformation, adding requirements for high-risk use cases (e.g., healthcare, legal guidance), expanding law enforcement use cases, and enhancing privacy protections by explicitly forbidding biometric data analysis for inferring characteristics or content censorship on behalf of governments.\\\",\\n      \\\"relevance\\\": 0.71,\\n      \\\"relevance_explanation\\\": \\\"Details updates to their Usage Policy, which outlines acceptable use of their products and includes important privacy and data-related stipulations.\\\"\\n    }\\n  ],\\n  \\\"error_message\\\": null\\n}\\n```\"}]"}}], "status": {"code": "STATUS_CODE_OK", "message": ""}, "attributes": {"mlflow.traceRequestId": "\"tr-e2996782b072e4cc88bcdc60d41faab1\"", "gen_ai.event.start_time": "2025-11-16T05:52:01.134153+00:00", "gen_ai.operation.name": "chat", "gen_ai.system": "strands-agents", "gen_ai.request.model": "gemini-2.5-flash", "gen_ai.event.end_time": "2025-11-16T05:52:24.790771+00:00", "gen_ai.usage.prompt_tokens": 44070, "gen_ai.usage.input_tokens": 44070, "gen_ai.usage.completion_tokens": 4932, "gen_ai.usage.output_tokens": 4932, "gen_ai.usage.total_tokens": 49002, "gen_ai.server.time_to_first_token": 15084, "mlflow.spanType": "\"CHAT_MODEL\"", "mlflow.spanInputs": "[{\"role\": \"user\", \"content\": [{\"text\": \"Anthropic\"}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_c330beab-4b76-4e55-b266-aa1f82a771ca\\\",\\n  \\\"query\\\": \\\"Anthropic official website\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Anthropic | CodeGPT\\\",\\n      \\\"url\\\": \\\"https://docs.codegpt.co/docs/tutorial-ai-providers/anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"On this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\\n\\\\nOn this page\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\n# Anthropic\\\\n\\\\nOfficial website [https://www.anthropic.com](https://www.anthropic.com)\\\\n\\\\nAnthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically.\\\\n\\\\n# Claude\\\\n\\\\nClaude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more. Personality, tone, and behavior can all be customized to your needs.\\\\n\\\\nTo access Claude, enter [https://www.anthropic.com/product](https://www.anthropic.com/product) and then select \\\\\\\"Request Access\\\\\\\".\\\\n\\\\nCheck your creditsPlease be aware of the credits; not having credits can result in a 500 error.\\\\n\\\\n## Connect your Anthropic account​\\\\n\\\\n- Create an account in Anthropic\\\\n- Go to your API Keys\\\\n- Generate and copy the API Key\\\\n- On the VSCode CodeGPT Extension, change the model on the chat, click on Manage Models button\\\\n- Pick the API Keys tab and select Anthropic as Provider\\\\n\\\\n- Paste API Key here, and click on `Connect`\\\\n- Click outside the options and ask to chat.\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n## API Errors​\\\\n\\\\nIf you are getting API errors, check the following link: [Anthropic API Errors](https://docs.anthropic.com/claude/reference/errors-and-rate-limits)\\\\n\\\\nCheck your credits\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\nRemove Key\\\\n\\\\nTo remove your API Key from CodeGPT:\\\\n\\\\n- Click again on the `Manage models` button.\\\\n- Click on \\\\\\\"Change connection settings\\\\\\\"\\\\n\\\\n- [Connect your Anthropic account](#connect-your-anthropic-account)\\\\n- [API Errors](#api-errors)\\\",\\n      \\\"description\\\": \\\"Official website https://www.anthropic.com · Anthropic is a San Francisco-based AI safety startup that develops techniques to ensure that AI systems align with human values and behave ethically. Claude can be a delightful company representative, a research assistant, a creative partner, a task Automator, and more.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4511,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.9087537058492818,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic - Wikipedia\\\",\\n      \\\"url\\\": \\\"https://en.wikipedia.org/wiki/Anthropic?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"Anthropic\\\\n\\\\n- [Article](https://en.wikipedia.org/wiki/Anthropic) - [Talk](https://en.wikipedia.org/wiki/Talk:Anthropic)\\\\n\\\\n**Anthropic PBC** is an American [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence) (AI) [startup](https://en.wikipedia.org/wiki/Startup) company founded in 2021. It has developed a family of [large language models](https://en.wikipedia.org/wiki/Large_language_model) (LLMs) named [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)). The company researches and develops AI to \\\\\\\"study their safety properties at the technological frontier\\\\\\\" and use this research to deploy [safe models](https://en.wikipedia.org/wiki/AI_safety) for the public.[6]\\\\n\\\\n| |\\\\n| --- |\\\\n| Company type | Private |\\\\n| --- | --- |\\\\n| Industry | Artificial intelligence |\\\\n| --- | --- |\\\\n| Founded | 2021; 4 years ago (2021) |\\\\n| --- | --- |\\\\n| Founders | Dario AmodeiDaniela AmodeiJared Kaplan[1]Jack Clark[2]Ben Mann[3] |\\\\n| --- | --- |\\\\n| Headquarters | 500 Howard Street,[4]San Francisco, California, U.S. |\\\\n| --- | --- |\\\\n| Key people | Dario Amodei (CEO)Daniela Amodei (President)Mike Krieger (CPO) |\\\\n| --- | --- |\\\\n| Products | ClaudeClaude Code |\\\\n| --- | --- |\\\\n| Number of employees | 1,300 (2025)[5] |\\\\n| --- | --- |\\\\n| Website | anthropic.com |\\\\n| --- | --- |\\\\n\\\\nAnthropic was founded by former members of [OpenAI](https://en.wikipedia.org/wiki/OpenAI), including siblings [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei) and [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei) (who serves as CEO).[7] In September 2023, [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) announced an investment of up to $4 billion. Google committed $2 billion the next month.[8][9][10] As of September 2025,[update] Anthropic is the fourth most valuable private company globally, valued at over $183 billion.[11][12]\\\\n\\\\n## Contents\\\\n\\\\n- [1 History](#History) 1.1 Founding and early development (2021–2022) 1.2 Major investments 1.3 Recruitment (2024) 1.4 Additional funding and partnerships (2025) - [2 Business structure](#Business_structure) 2.1 Key employees - [3 Projects](#Projects) 3.1 Claude 3.2 U.S. military and intelligence 3.3 Education-related projects - [4 Research](#Research) 4.1 Constitutional AI 4.2 Interpretability 4.3 Automation - [5 Legal issues](#Legal_issues) - [6 See also](#See_also) - [7 References](#References) - [8 External links](#External_links)\\\\n\\\\n## History\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=1)\\\\n\\\\n[Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei), co-founder\\\\n\\\\n### Founding and early development (2021–2022)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=2)\\\\n\\\\nAnthropic was founded in 2021 by seven former employees of OpenAI, including siblings Daniela Amodei and Dario Amodei, the latter of whom was OpenAI's Vice President of Research.[13][14]\\\\n\\\\nIn April 2022, Anthropic announced it had received $580 million in funding,[15] including a $500 million investment from [FTX](https://en.wikipedia.org/wiki/FTX) under the leadership of [Sam Bankman-Fried](https://en.wikipedia.org/wiki/Sam_Bankman-Fried).[16][3]\\\\n\\\\nIn the summer of 2022, Anthropic finished training the first version of Claude but did not release it, citing the need for further internal safety testing and a desire to avoid initiating a potentially hazardous race to develop increasingly powerful AI systems.[17]\\\\n\\\\n### Major investments\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=3)\\\\n\\\\nIn September 2023, Amazon announced a partnership with Anthropic. Amazon became a minority [stakeholder](https://en.wikipedia.org/wiki/Stakeholder_(corporate)) by initially investing $1.25 billion and planning a total investment of $4 billion.[8] The remaining $2.75 billion was invested in March 2024.[18] In November 2024, Amazon invested another $4 billion, doubling its total investment.[19] As part of the deal, Anthropic uses [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) (AWS) as its primary cloud provider and makes its AI models available to AWS customers.[8]\\\\n\\\\nIn October 2023, Google invested $500 million in Anthropic and committed to an additional $1.5 billion over time.[10] In March 2025, Google agreed to invest another $1 billion in Anthropic.[20]\\\\n\\\\n### Recruitment (2024)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=4)\\\\n\\\\nIn February 2024, Anthropic hired former [Google Books](https://en.wikipedia.org/wiki/Google_Books) head of partnerships Tom Turvey, and tasked him with obtaining \\\\\\\"all the books in the world\\\\\\\".[21] The company then began using [destructive book scanning](https://en.wikipedia.org/wiki/Book_scanning#Destructive_scanning_methods) to digitize \\\\\\\"millions\\\\\\\" of books to train [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)).[21]\\\\n\\\\nIn 2024, Anthropic attracted several notable employees from OpenAI, including [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike), [John Schulman](https://en.wikipedia.org/wiki/John_Schulman), and Durk Kingma.[22]\\\\n\\\\n### Additional funding and partnerships (2025)\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=5)\\\\n\\\\nAnthropic raised $3.5 billion in a Series E funding round in March 2025, achieving a post-money valuation of $61.5 billion, led by [Lightspeed Venture Partners](https://en.wikipedia.org/wiki/Lightspeed_Venture_Partners) with participation from several major investors.[23][24] In March, [Databricks](https://en.wikipedia.org/wiki/Databricks) and Anthropic announced that Claude would be integrated into the Databricks Data Intelligence Platform.[25][26]\\\\n\\\\nIn May 2025, the company announced Claude 4, introducing both Claude Opus 4 and Claude Sonnet 4 with improved coding capabilities and other new features.[27] It also introduced new API capabilities, including the [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol) (MCP) connector.[27] In May, Anthropic launched a [web search](https://en.wikipedia.org/wiki/Web_search) API that enables Claude to access real-time information from the internet.[28] Claude Code, Anthropic's coding assistant, transitioned from research preview to general availability, featuring integrations with [VS Code](https://en.wikipedia.org/wiki/VS_Code) and [JetBrains IDEs](https://en.wikipedia.org/wiki/JetBrains#Products) and support for GitHub Actions.[27]\\\\n\\\\nIn September 2025, Anthropic completed a Series F funding round, raising $13 billion at a post-money valuation of $183 billion. The round was co-led by [Iconiq Capital](https://en.wikipedia.org/wiki/Iconiq_Capital), [Fidelity Management & Research](https://en.wikipedia.org/wiki/Fidelity_Management_%26_Research), and Lightspeed Venture Partners, with participation from the [Qatar Investment Authority](https://en.wikipedia.org/wiki/Qatar_Investment_Authority) and other investors.[29][30] The same month, Anthropic announced that it would stop selling its products to groups majority-owned by Chinese, Russian, Iranian, or North Korean entities due to national security concerns.[31]\\\\n\\\\nIn October 2025, Anthropic announced a cloud partnership with Google, giving it access to up to one million of Google's custom [Tensor Processing Units](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) (TPUs). According to Anthropic, the partnership will bring more than one gigawatt of AI compute capacity online by 2026.[32]\\\\n\\\\n## Business structure\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=6)\\\\n\\\\nAnthropic's corporate structure\\\\n\\\\nAccording to Anthropic, its goal is to research AI systems' safety and reliability.[33] The Amodei siblings were among those who left OpenAI due to directional differences.[14]\\\\n\\\\nAnthropic incorporated itself as a [Delaware](https://en.wikipedia.org/wiki/Delaware) [public-benefit corporation](https://en.wikipedia.org/wiki/Benefit_corporation) (PBC), which enables directors to balance stockholders' financial interests with its public benefit purpose.[34]\\\\n\\\\nAnthropic's \\\\\\\"Long-Term Benefit Trust\\\\\\\" is a purpose trust for \\\\\\\"the responsible development and maintenance of advanced AI for the long-term benefit of humanity\\\\\\\". It holds Class T shares in the PBC, which allow it to elect directors to Anthropic's board.[35][36] As of October 2025, the members of the Trust are Neil Buddy Shah, Kanika Bahl, Zach Robinson and [Richard Fontaine](https://en.wikipedia.org/wiki/Richard_Fontaine).[37]\\\\n\\\\nInvestors include [Amazon](https://en.wikipedia.org/wiki/Amazon_(company)) at $8 billion,[38] [Google](https://en.wikipedia.org/wiki/Google) at $2 billion,[10] and [Menlo Ventures](https://en.wikipedia.org/wiki/Menlo_Ventures) at $750 million.[39]\\\\n\\\\n### Key employees\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=7)\\\\n\\\\n- [Dario Amodei](https://en.wikipedia.org/wiki/Dario_Amodei): co-founder and CEO - [Daniela Amodei](https://en.wikipedia.org/wiki/Daniela_Amodei): co-founder and President - [Mike Krieger](https://en.wikipedia.org/wiki/Mike_Krieger): Chief Product Officer[40][41] - Jack Clark: co-Founder and Head of Policy - Tom Brown: co-Founder and Head of Core Resources - Krishna Rao: Chief Financial Officer - Jason Clinton: Chief Information Security Officer - [Jan Leike](https://en.wikipedia.org/wiki/Jan_Leike): co-lead of the Alignment Science team, ex-OpenAI alignment researcher[42][43]\\\\n\\\\n## Projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=8)\\\\n\\\\n### Claude\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=9)\\\\n\\\\nMain article: [Claude (language model)](https://en.wikipedia.org/wiki/Claude_(language_model))\\\\n\\\\nClaude incorporates \\\\\\\"Constitutional AI\\\\\\\" to set safety guidelines for the model's output.[44] The name \\\\\\\"Claude\\\\\\\" was chosen as a reference to mathematician [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon), and as a masculine name to contrast with the feminine names of AI assistants such as [Alexa](https://en.wikipedia.org/wiki/Amazon_Alexa), [Siri](https://en.wikipedia.org/wiki/Siri), and [Cortana](https://en.wikipedia.org/wiki/Cortana_(virtual_assistant)).[3]\\\\n\\\\nIn March 2023, Anthropic released two versions of its model, Claude and Claude Instant, the latter being more lightweight.[45][46][47] The next iteration, Claude 2, was launched in July 2023.[48] Unlike Claude, which was only available to select users, Claude 2 is available for public use.[49]\\\\n\\\\nClaude 3 was released in March 2024, with three language models: Opus, Sonnet, and Haiku.[50][51] The Opus model is the largest. According to Anthropic, it outperformed OpenAI's [GPT-4](https://en.wikipedia.org/wiki/GPT-4) and GPT-3.5 and Google's Gemini Ultra in benchmark tests at the time. Sonnet and Haiku are Anthropic's medium- and small-sized models, respectively. All three models accept image input.[50] Amazon has added Claude 3 to its cloud AI service Bedrock.[52]\\\\n\\\\nIn May 2024, Anthropic announced the Claude Team plan, its first enterprise offering for Claude, and Claude iOS app.[53]\\\\n\\\\nIn June 2024, Anthropic released Claude 3.5 Sonnet, which demonstrated significantly improved performance on benchmarks compared to the larger Claude 3 Opus, notably in coding, multistep workflows, chart interpretation, and text extraction from images. Released alongside 3.5 Sonnet was the new Artifacts capability, in which Claude generates content in a dedicated window and renders interactive elements such as websites or [SVGs](https://en.wikipedia.org/wiki/SVG) in real time.[54]\\\\n\\\\nIn October 2024, Anthropic released an improved version of Claude 3.5, along with a beta feature called \\\\\\\"Computer use\\\\\\\", which enables Claude to take screenshots, click, and type text.[55]\\\\n\\\\nIn November 2024, [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) announced a partnership with Anthropic and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to give U.S. intelligence and defense agencies access to Claude 3 and 3.5. According to Palantir, this was the first time that Claude would be used in \\\\\\\"[classified](https://en.wikipedia.org/wiki/Classified_information) environments\\\\\\\".[56]\\\\n\\\\nIn December 2024, Claude 3.5 Haiku was made available to all users on web and mobile platforms.[57]\\\\n\\\\nIn February 2025, Claude 3.7 Sonnet was introduced to all paid users. It is a \\\\\\\"hybrid [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model)\\\\\\\" model (one that responds directly to simple queries, while taking more time for complex problems).[58][59]\\\\n\\\\nIn May 2025, Claude 4 Opus and Sonnet were introduced. With these models, Anthropic also introduced Extended thinking with tool use and the ability to use tools in parallel.[60]\\\\n\\\\nIn August 2025, Claude Opus 4.1 was introduced.[61]\\\\n\\\\nIn September 2025, Claude Sonnet 4.5 was released[62] and, in October 2025, Claude Haiku 4.5 was released.[63]\\\\n\\\\nAnthropic is currently researching if Claude is capable of being introspective and can reason why it came to certain conclusions.[64]\\\\n\\\\n### U.S. military and intelligence\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=10)\\\\n\\\\nIn November 2024, Anthropic partnered with [Palantir](https://en.wikipedia.org/wiki/Palantir_Technologies) and [Amazon Web Services](https://en.wikipedia.org/wiki/Amazon_Web_Services) to provide the Claude model to U.S. intelligence and defense agencies.[65][66] In June 2025, Anthropic announced a \\\\\\\"Claude Gov\\\\\\\" model. *Ars Technica* reported that as of June 2025 it was in use at multiple U.S. national security agencies.[67]\\\\n\\\\nIn July 2025, the [United States Department of Defense](https://en.wikipedia.org/wiki/United_States_Department_of_Defense) announced that Anthropic had received a $200 million contract for AI in the military, along with Google, OpenAI, and [xAI](https://en.wikipedia.org/wiki/XAI_(company)).[68]\\\\n\\\\n### Education-related projects\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=11)\\\\n\\\\nIn August 2025, Anthropic launched a Higher Education Advisory Board, chaired by former [Yale University](https://en.wikipedia.org/wiki/Yale_University) president and former [Coursera](https://en.wikipedia.org/wiki/Coursera) CEO Rick Levin.[69]\\\\n\\\\nAnthropic partnered with Iceland's Ministry of Education and Children in 2025 to allow teachers from all over the country, including remote areas, to access Claude and integrate AI into daily teaching.[70]\\\\n\\\\n## Research\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=12)\\\\n\\\\n### Constitutional AI\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=13)\\\\n\\\\nMain article: [Constitutional AI](https://en.wikipedia.org/wiki/Constitutional_AI)\\\\n\\\\nAccording to Anthropic, Constitutional AI (CAI) is a framework developed to align AI systems with human values and ensure that they are helpful, harmless, and honest.[13][71] Within this framework, humans provide a set of rules describing the desired behavior of the AI system, known as the \\\\\\\"constitution\\\\\\\".[71] The AI system evaluates the generated output and then adjusts the AI models to better fit the constitution.[71] The self-reinforcing process aims to avoid harm, respect preferences, and provide true information.[71]\\\\n\\\\nSome of the principles of Claude 2's constitution are derived from documents such as the [1948 Universal Declaration of Human Rights](https://en.wikipedia.org/wiki/Universal_Declaration_of_Human_Rights) and Apple's terms of service.[48] For example, one rule from the UN Declaration applied in Claude 2's CAI states \\\\\\\"Please choose the response that most supports and encourages freedom, equality and a sense of brotherhood.\\\\\\\"[48]\\\\n\\\\n### Interpretability\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=14)\\\\n\\\\nAnthropic also publishes research on the [interpretability](https://en.wikipedia.org/wiki/Interpretability_(machine_learning)) of [machine learning](https://en.wikipedia.org/wiki/Machine_learning) systems, focusing on the [transformer](https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)) architecture.[13][72][73]\\\\n\\\\nPart of Anthropic's research aims to be able to automatically identify \\\\\\\"features\\\\\\\" in [generative pretrained transformers](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer) like Claude. In a [neural network](https://en.wikipedia.org/wiki/Neural_network_(machine_learning)), a feature is a pattern of neural activations that corresponds to a concept. In 2024, using a compute-intensive technique called \\\\\\\"[dictionary learning](https://en.wikipedia.org/wiki/Dictionary_learning)\\\\\\\", Anthropic was able to identify millions of features in Claude, including for example one associated with the [Golden Gate Bridge](https://en.wikipedia.org/wiki/Golden_Gate_Bridge). Enhancing the ability to identify and edit features is expected to have significant safety implications.[74][75][76]\\\\n\\\\nIn March 2025, research by Anthropic suggested that multilingual LLMs partially process information in a conceptual space before converting it to the appropriate language. It also found evidence that LLMs can sometimes plan ahead. For example, when writing poetry, Claude identifies potential rhyming words before generating a line that ends with one of these words.[77][78]\\\\n\\\\n### Automation\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=15)\\\\n\\\\nIn September 2025, Anthropic released a report saying that businesses primarily use AI for automation rather than collaboration, with three-quarters of companies that work with Claude using it for “full task delegation\\\\\\\".[79] Earlier in the year, CEO Dario Amodei predicted that AI would wipe out white-collar jobs, especially entry-level jobs in finance, law, and consulting.[80][81]\\\\n\\\\n## Legal issues\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=16)\\\\n\\\\nOn October 18, 2023, Anthropic was sued by [Concord](https://en.wikipedia.org/wiki/Concord_Music_Group), [Universal](https://en.wikipedia.org/wiki/Universal_Music_Group), [ABKCO](https://en.wikipedia.org/wiki/ABKCO_Records), and other music publishers for, per the complaint, \\\\\\\"systematic and widespread infringement of their copyrighted song lyrics.\\\\\\\"[82][83][84] They alleged that the company used copyrighted material without permission in the form of song lyrics.[85] The plaintiffs asked for up to $150,000 for each work infringed upon by Anthropic, citing infringement of copyright laws.[85] In the lawsuit, the plaintiffs support their allegations of copyright violations by citing several examples of Anthropic's Claude model outputting copied lyrics from songs such as [Katy Perry](https://en.wikipedia.org/wiki/Katy_Perry)'s \\\\\\\"Roar\\\\\\\" and [Gloria Gaynor](https://en.wikipedia.org/wiki/Gloria_Gaynor)'s \\\\\\\"I Will Survive\\\\\\\".[85] Additionally, the plaintiffs alleged that even given some prompts that did not directly state a song name, the model responded with modified lyrics based on original work.[85]\\\\n\\\\nOn January 16, 2024, Anthropic claimed that the music publishers were not unreasonably harmed and that the examples noted by plaintiffs were merely bugs.[86]\\\\n\\\\nIn August 2024, a class-action lawsuit was filed against Anthropic in California for alleged copyright infringement. The suit claims Anthropic fed its LLMs with pirated copies of the authors' work, including from participants [Kirk Wallace Johnson](https://en.wikipedia.org/wiki/Kirk_W._Johnson), [Andrea Bartz](https://en.wikipedia.org/wiki/Andrea_Bartz), and [Charles Graeber](https://en.wikipedia.org/wiki/Charles_Graeber).[87] On June 23, 2025, the [United States District Court for the Northern District of California](https://en.wikipedia.org/wiki/United_States_District_Court_for_the_Northern_District_of_California) granted summary judgment for Anthropic that the use of digital copies of the plaintiffs' works (inter alia) for the purpose of training Anthropic's LLMs was a [fair use](https://en.wikipedia.org/wiki/Fair_use). But it found that Anthropic had used millions of pirated library copies and that such use of pirated copies could not be a fair use. Therefore the case was ordered to go to trial on the pirated copies used to create Anthropic's central library and the resulting damages.[88] In September 2025, Anthropic agreed to pay authors $1.5 billion to settle the case, amounting to $3,000 per book plus interest. The proposed settlement, pending judge approval, stands as the largest copyright resolution in U.S. history.[89][90]\\\\n\\\\nIn June 2025, [Reddit](https://en.wikipedia.org/wiki/Reddit) sued Anthropic, alleging that Anthropic is scraping data from the website in violation of Reddit's user agreement.[91]\\\\n\\\\n## See also\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=17)\\\\n\\\\n- [Apprenticeship learning](https://en.wikipedia.org/wiki/Apprenticeship_learning) - [AI alignment](https://en.wikipedia.org/wiki/AI_alignment) - [Friendly AI](https://en.wikipedia.org/wiki/Friendly_AI) - [Model Context Protocol](https://en.wikipedia.org/wiki/Model_Context_Protocol)\\\\n\\\\n## References\\\\n\\\\n[edit](https://en.wikipedia.org/w/index.php?title=Anthropic&action=edit&section=18)\\\\n\\\\n1. ^ Lin, Belle. \\\\\\\"Google and Anthropic Are Selling Generative AI to Businesses, Even as They Address Its Shortcomings\\\\\\\". WSJ. Retrieved April 11, 2024. 2. ^ Nellis, Stephen (May 9, 2023). \\\\\\\"Alphabet-backed Anthropic outlines the moral values behind its AI bot\\\\\\\". Reuters. Archived from the original on June 5, 2023. Retrieved June 4, 2023. 3. ^ a b c Roose, Kevin (July 11, 2023). \\\\\\\"Inside the White-Hot Center of A.I. Doomerism\\\\\\\". The New York Times. Archived from the original on July 12, 2023. Retrieved July 13, 2023. 4. ^ Waxmann, Laura (September 22, 2025). \\\\\\\"Growing tech firm Anthropic expands S.F. presence with large office lease in SoMa\\\\\\\". San Francisco Chronicle. Archived from the original on September 22, 2025. 5. ^ Robison, Kylie (May 23, 2025). \\\\\\\"Inside Anthropic's First Developer Day, Where AI Agents Took Center Stage\\\\\\\". Wired. Archived from the original on May 23, 2025. 6. ^ \\\\\\\"ChatGPT must be regulated and A.I. 'can be used by bad actors,' warns OpenAI's CTO\\\\\\\". Fortune. February 5, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023 – via finance.yahoo.com. 7. ^ \\\\\\\"Anthropic | Company Overview & News\\\\\\\". Forbes. Retrieved April 11, 2024. 8. ^ a b c Dastin, Jeffrey (September 28, 2023). \\\\\\\"Amazon steps up AI race with Anthropic investment\\\\\\\". Reuters. Archived from the original on November 5, 2023. Retrieved October 2, 2023. 9. ^ Coldewey, Devin (March 27, 2024). \\\\\\\"Amazon doubles down on Anthropic, completing its planned $4B investment\\\\\\\". TechCrunch. Retrieved March 28, 2024. 10. ^ a b c Hu, Krystal (October 27, 2023). \\\\\\\"Google agrees to invest up to $2 billion in OpenAI rival Anthropic\\\\\\\". Reuters. Archived from the original on November 2, 2023. Retrieved October 30, 2023. 11. ^ Chernova, Yuliya (September 2, 2025). \\\\\\\"Anthropic Valuation Hits $183 Billion in New $13 Billion Funding Round\\\\\\\". The Wall Street Journal. ISSN 0099-9660. Retrieved September 6, 2025. 12. ^ Bellan, Rebecca (September 2, 2025). \\\\\\\"Anthropic raises $13B Series F at $183B valuation\\\\\\\". TechCrunch. Retrieved September 4, 2025. 13. ^ a b c Henshall, Will; Perrigo, Billy (September 7, 2023). \\\\\\\"TIME100 AI 2023: Dario and Daniela Amodei\\\\\\\". Time. Retrieved March 5, 2024. 14. ^ a b \\\\\\\"As Anthropic seeks billions to take on OpenAI, 'industrial capture' is nigh. Or is it?\\\\\\\". VentureBeat. April 7, 2023. Archived from the original on May 24, 2023. Retrieved May 24, 2023. 15. ^ \\\\\\\"Google invests $300 million in Anthropic as race to compete with ChatGPT heats up\\\\\\\". VentureBeat. February 3, 2023. Archived from the original on February 6, 2023. Retrieved February 6, 2023. 16. ^ Sigalos, MacKenzie (March 25, 2024). \\\\\\\"FTX estate selling majority stake in AI startup Anthropic for $884 million, with bulk going to UAE\\\\\\\". CNBC. Retrieved April 11, 2024. 17. ^ Perrigo, Billy (May 30, 2024). \\\\\\\"Inside Anthropic, the AI Company Betting That Safety Can Be a Winning Strategy\\\\\\\". Time. Retrieved June 2, 2024. 18. ^ \\\\\\\"Amazon and Anthropic deepen their shared commitment to advancing generative AI\\\\\\\". Amazon. March 27, 2024. Retrieved September 24, 2025. 19. ^ Bajwa, Arsheeya; Hu, Krystal; Hu, Krystal (November 22, 2024). \\\\\\\"Amazon doubles down on AI startup Anthropic with another $4 bln\\\\\\\". Reuters. Retrieved September 24, 2025. 20. ^ Field, Hayden (January 22, 2025). \\\\\\\"Google agrees to new $1 billion investment in Anthropic\\\\\\\". CNBC. Retrieved September 24, 2025. 21. ^ a b Edwards, Benj (June 25, 2025). \\\\\\\"Anthropic destroyed millions of print books to build its AI models\\\\\\\". Ars Technica. Retrieved June 26, 2025. 22. ^ Wiggers, Kyle (October 1, 2024). \\\\\\\"Anthropic hires OpenAI co-founder Durk Kingma\\\\\\\". TechCrunch. Retrieved October 5, 2024. 23. ^ \\\\\\\"Anthropic raises Series E at $61.5B post-money valuation\\\\\\\". Anthropic. March 3, 2025. Retrieved July 27, 2025. 24. ^ \\\\\\\"Anthropic reportedly ups its next funding round to $3.\\\",\\n      \\\"description\\\": \\\"Anthropic was founded by former members of OpenAI, including siblings Daniela Amodei and Dario Amodei (who serves as CEO). In September 2023, Amazon announced an investment of up to $4 billion. Google committed $2 billion the next month.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 24997,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/d/da/Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg/250px-Dario_Amodei_at_TechCrunch_Disrupt_2023_01.jpg\\\",\\n        \\\"1\\\": \\\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Anthropic_corporate_structure.svg/250px-Anthropic_corporate_structure.svg.png\\\",\\n        \\\"2\\\": \\\"https://upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/20px-OOjs_UI_icon_edit-ltr-progressive.svg.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.8464241025144909,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Home \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Claude Sonnet 4.5\\\\n\\\\nIntroducing the best model in the world for agents, coding, and computer use.\\\\n\\\\n### Learn more\\\\n\\\\n#### Introducing Claude Haiku 4.5\\\\n\\\\n#### Managing context on the Claude Developer Platform\\\\n\\\\n#### Model details\\\\n\\\\n“\\\\n\\\\nWhen you’re talking to a large language model, what exactly is it that you’re talking to?\\\\n\\\\nAt Anthropic, we build AI to serve humanity’s long-term well-being.\\\\n\\\\n## ‍\\\\n\\\\nWhile no one can foresee every outcome AI will have on society, we do know that designing powerful technologies requires both bold steps forward and intentional pauses to consider the effects.\\\\n\\\\nThat’s why we focus on building tools with human benefit at their foundation, like Claude. Through our daily research, policy work, and product design, we aim to show what responsible AI development looks like in practice.\\\\n\\\\n### Core Views on AI Safety\\\\n\\\\n### Anthropic’s Responsible Scaling Policy\\\\n\\\\n### Anthropic Academy: Learn to build with Claude\\\\n\\\\nFeatured\\\\n\\\\n- Introducing Haiku 4.5AnnouncementsOct 15, 2025\\\\n\\\\n### Introducing Haiku 4.5\\\\n\\\\n- Introducing Sonnet 4.5AnnouncementsSep 29, 2025\\\\n\\\\n### Introducing Sonnet 4.5\\\\n\\\\n- Anthropic Economic IndexEconomic Research Sep 15, 2025\\\\n\\\\n### Anthropic Economic Index\\\\n\\\\n- Claude Opus 4.1AnnouncementsAug 05, 2025\\\\n\\\\n### Claude Opus 4.1\\\\n\\\\n- Project VendPolicyJun 26, 2025\\\\n\\\\n### Project Vend\\\\n\\\\n- Agentic MisalignmentAlignment Jun 20, 2025\\\\n\\\\n### Agentic Misalignment\\\\n\\\\n- Tracing the thoughts of a large language modelInterpretability Mar 27, 2025\\\\n\\\\n### Tracing the thoughts of a large language model\\\\n\\\\n## Want to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is an AI safety and research company that&#x27;s working to build reliable, interpretable, and steerable AI systems.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1560,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68b20290e3cdc5e21abc263f_266bcdf860100973b911d000b9f9beb7_maxresdefault-1.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7115991641957488,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Company \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/company?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"## Our Purpose\\\\n\\\\nWe believe AI will have a vast impact on the world. Anthropic is dedicated to building systems that people can rely on and generating research about the opportunities and risks of AI.\\\\n\\\\n- We Build Safer SystemsWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n### We Build Safer Systems\\\\n\\\\nWe aim to build frontier AI systems that are reliable, interpretable, and steerable. We conduct frontier research, develop and apply a variety of safety techniques, and deploy the resulting systems via a set of partnerships and products.\\\\n\\\\n- Safety Is a ScienceWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n### Safety Is a Science\\\\n\\\\nWe treat AI safety as a systematic science, conducting research, applying it to our products, feeding those insights back into our research, and regularly sharing what we learn with the world along the way.\\\\n\\\\n- InterdisciplinaryAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n### Interdisciplinary\\\\n\\\\nAnthropic is a collaborative team of researchers, engineers, policy experts, business leaders and operators, who bring our experience from many different domains to our work.\\\\n\\\\n- AI Companies are One Piece of a Big PuzzleAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n### AI Companies are One Piece of a Big Puzzle\\\\n\\\\nAI has the potential to fundamentally change how the world works. We view ourselves as just one piece of this evolving puzzle. We collaborate with civil society, government, academia, nonprofits and industry to promote safety industry-wide.\\\\n\\\\n## The Team\\\\n\\\\nWe’re a team of researchers, engineers, policy experts and operational leaders, with experience spanning a variety of disciplines, all working together to build reliable and understandable AI systems.\\\\n\\\\n- ResearchWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n### Research\\\\n\\\\nWe conduct frontier AI research across a variety of modalities, and explore novel and emerging safety research areas from interpretability to RL from human feedback to policy and societal impacts analysis.\\\\n\\\\n- PolicyWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n### Policy\\\\n\\\\nWe think about the impacts of our work and strive to communicate what we’re seeing at the frontier to policymakers and civil society in the US and abroad to help promote safe and reliable AI.\\\\n\\\\n- ProductWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n### Product\\\\n\\\\nWe translate our research into tangible, practical tools like Claude that benefit businesses, nonprofits and civil society groups and their clients and people around the globe.\\\\n\\\\n- OperationsOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n### Operations\\\\n\\\\nOur people, finance, legal, and recruiting teams are the human engines that make Anthropic go. We’ve had previous careers at NASA, startups, and the armed forces and our diverse experiences help make Anthropic a great place to work (and we love plants!).\\\\n\\\\n## What we value and how we act\\\\n\\\\nEvery day, we make critical decisions that inform our ability to achieve our mission. Shaping the future of AI and, in turn, the future of our world is a responsibility and a privilege. Our values guide how we work together, the decisions we make, and ultimately how we show up for each other and work toward our mission.\\\\n\\\\n- 01Act for the global good.We strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n01\\\\n\\\\n### Act for the global good.\\\\n\\\\nWe strive to make decisions that maximize positive outcomes for humanity in the long run. This means we’re willing to be very bold in the actions we take to ensure our technology is a robustly positive force for good. We take seriously the task of safely guiding the world through a technological revolution that has the potential to change the course of human history, and are committed to helping make this transition go well.\\\\n\\\\n- 02Hold light and shade.AI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n02\\\\n\\\\n### Hold light and shade.\\\\n\\\\nAI has the potential to pose unprecedented risks to humanity if things go badly. It also has the potential to create unprecedented benefits for humanity if things go well. We need shade to understand and protect against the potential for bad outcomes. We need light to realize the good outcomes.\\\\n\\\\n- 03Be good to our users.At Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n03\\\\n\\\\n### Be good to our users.\\\\n\\\\nAt Anthropic, we define “users” broadly. Users are our customers, policy-makers, Ants, and anyone impacted by the technology we build or the actions we take. We cultivate generosity and kindness in all our interactions—with each other, with our users, and with the world at large. Going above and beyond for each other, our customers, and all of the people affected by our technology is meeting expectations.\\\\n\\\\n- 04Ignite a race to the top on safety.As a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n04\\\\n\\\\n### Ignite a race to the top on safety.\\\\n\\\\nAs a safety-first company, we believe that building reliable, trustworthy, and secure systems is our collective responsibility - and the market agrees. We work to inspire a ‘race to the top’ dynamic where AI developers must compete to develop the most safe and secure AI systems. We want to constantly set the industry bar for AI safety and security and drive others to do the same.\\\\n\\\\n- 05Do the simple thing that works.We take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n05\\\\n\\\\n### Do the simple thing that works.\\\\n\\\\nWe take an empirical approach to problems and care about the size of our impact and not the sophistication of our methods. This doesn’t mean we throw together haphazard solutions. It means we try to identify the simplest solution and iterate from there. We don’t invent a spaceship if all we need is a bicycle.\\\\n\\\\n- 06Be helpful, honest, and harmless.Anthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n06\\\\n\\\\n### Be helpful, honest, and harmless.\\\\n\\\\nAnthropic is a high-trust, low-ego organization. We communicate kindly and directly, assuming good intentions even in disagreement. We are thoughtful about our actions, avoiding harm and repairing relationships when needed. Everyone contributes, regardless of role. If something urgently needs to be done, the right person to do it is probably you!\\\\n\\\\n- 07Put the mission first.At the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n07\\\\n\\\\n### Put the mission first.\\\\n\\\\nAt the end of the day, the mission is what we’re all here for. It gives us a shared purpose and allows us to act swiftly together, rather than being pulled in multiple directions by competing goals. It engenders trust and collaboration and is the final arbiter in our decisions. When it comes to our mission, none of us are bystanders. We each take personal ownership over making our mission successful.\\\\n\\\\n## Governance\\\\n\\\\nAnthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here. Current members of the Board and the Long-Term Benefit Trust (LTBT) are listed below.\\\\n\\\\nAnthropic Board of DirectorsDario Amodei, Daniela Amodei, Yasmin Razavi, Jay Kreps, and Reed Hastings.\\\\n\\\\nLTBT TrusteesNeil Buddy Shah, Kanika Bahl, Zach Robinson, and Richard Fontaine.\\\\n\\\\nWant to help us build the future of safe AI?\\\",\\n      \\\"description\\\": \\\"Anthropic is a Public Benefit Corporation, whose purpose is the responsible development and maintenance of advanced AI for the long-term benefit of humanity. Our Board of Directors is elected by stockholders and our Long-Term Benefit Trust, as explained here.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 10735,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/68309ab48369f7ad9b4a40e1_open-graph.jpg\\\",\\n        \\\"1\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62091e3b46dac5428888_research.webp\\\",\\n        \\\"2\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d620978e48d8861871f7d_policy.webp\\\",\\n        \\\"3\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/681d62094dad1585c366d2da_product.webp\\\",\\n        \\\"4\\\": \\\"https://cdn.prod.website-files.com/67ce28cfec624e2b733f8a52/67d3c039bc939998a7c43b37_work-with-anthropic.webp\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6950796973102905,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropics Technology Ltd | World Leaders in AI Innovation\\\",\\n      \\\"url\\\": \\\"https://www.anthropics.com/?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# A WORLD LEADER IN\\\\n\\\\n# AI IMAGE EDITING\\\\n\\\\n# WHAT WE DO\\\\n\\\\nAt Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression. Specializing in cutting-edge photo editing software, we empower users to bring their visions to life with seamless, intuitive tools.\\\\n\\\\nOur mission is simple: to blend innovation with imagination, transforming the way people see themselves and their possibilities.\\\\n\\\\n# OUR PRODUCTS\\\\n\\\\n### BRIDELY\\\\n\\\\nOur latest innovation - an AI-driven virtual wedding dress try-on experience - redefines how brides-to-be explore and envision their perfect look. By combining advanced imaging technology with a passion for personalization, we make dream dresses more accessible, decision-making easier, and the journey to \\\\\\\"I do\\\\\\\" more magical.\\\\n\\\\n# Desktop Products\\\\n\\\\nOur suite of editing products gives you full control, transforming your photography.\\\\n\\\\n### PortraitPro\\\\n\\\\nPortraitPro is the world’s best-selling professional retouching software. Using Artificial Intelligence, PortraitPro enhances every aspect of a portrait for beautifully natural results.\\\\n\\\\n### PortraitPro Body\\\\n\\\\nIncorporating state of the art body editing technology, PortraitPro Body is the world’s first software that enables full-body portraits to be realistically modified, simply by moving sliders.\\\\n\\\\n### LandscapePro\\\\n\\\\nWith AI-based controls that adapt to the features of the photo, LandscapePro is the world’s first intelligent landscape editor.\\\\n\\\\n### Smart Photo Editor\\\\n\\\\nSmart Photo Editor is the world’s first community-based photo editor, with an ever growing library of sophisticated effects produced by its enthusiastic users.\\\\n\\\\n### Powering Creativity\\\\n\\\\nEstablished in 1998, the company is a global leader in imaging technologies from desktop photo-editing software to white label retail solutions and now has begun partnering with amazing fashion industry brands to develop revolutionary solutions for online fashion e-Commerce.\\\",\\n      \\\"description\\\": \\\"At Anthropics, we harness the power of AI to revolutionize digital creativity and personal expression.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1936,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropics.com/img/home/portraitpro.jpg\\\",\\n        \\\"1\\\": \\\"https://www.anthropics.com/img/home/portraitprobody.jpg\\\",\\n        \\\"2\\\": \\\"https://www.anthropics.com/img/home/landscapepro.jpg\\\",\\n        \\\"3\\\": \\\"https://www.anthropics.com/img/home/smartphotoeditor.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6806646179072172,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 43739\\n}\"}]}}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_5bedea7b-0d02-4626-9485-4db56a2b8744\\\",\\n  \\\"query\\\": \\\"site:anthropic.com terms of service\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"You’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below. ... Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.749548504986343,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"- “Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n“Provider” definition. We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability.\\\\n\\\\n- Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\\n\\\\nSoftware updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"description\\\": \\\"We updated the definition of “Providers” to include our affiliates, licensors, distributors, and service providers. We also clarified that our Providers are intended third-party beneficiaries of specified disclaimers and limitations of liability. Software updates. We clarified our terms about software updates, including that we may offer automatic updates to our software to ensure our users have access to the latest version.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.722664176602502,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Updating our Usage Policy\\\\n\\\\nToday, we're updating the policies that protect our users and ensure our products and services are used responsibly. Our goal with these updates is to clarify which applications of our products are and are not allowed so our policies are clear and easy for users to understand.\\\\n\\\\nPreviously, we referred to this set of policies as our Acceptable Use Policy. We are now updating the name to Usage Policy. These changes will take effect on June 6, 2024.\\\\n\\\\nWe consider a number of factors when updating these policies, including the practical applications of our models, an evolving landscape of laws and regulations, and the advancing capability of our models and tools for detecting and preventing potential misuse. Our Usage Policy plays a crucial role in how we execute our safety mission.\\\\n\\\\nIn the following section, we will outline the significant updates to our Usage Policy.\\\\n\\\\n### Streamlining our policies\\\\n\\\\nPreviously, our Acceptable Use Policy was divided into two sections: \\\\\\\"Prohibited Uses\\\\\\\" and \\\\\\\"Prohibited Business Cases.\\\\\\\" To provide more clarity and emphasize that certain rules apply to all users of our products—both businesses and consumers—we’ve consolidated these sections into a single set of guidelines called \\\\\\\"Universal Usage Standards.\\\\\\\"\\\\n\\\\n### Clarifying our policies on election integrity and misinformation\\\\n\\\\nWe have consistently prohibited the use of our products for political lobbying and campaigning. Our updated policy provides clearer definitions of the activities that fall under each category. For instance, we explicitly state that our products cannot be used to promote or advocate for a specific candidate, party, issue, or position. We also prohibit the use of our products for political activities such as soliciting votes or financial contributions.\\\\n\\\\n### Adding requirements for high-risk use cases\\\\n\\\\nOur products can be used to provide information and perform analysis to help organizations make decisions. However, in some cases, these decisions may have significant consequences for individuals and require specialized expertise. We have defined these specific circumstances as high-risk use cases, which include integrations of our API that affect healthcare decisions and legal guidance. To address these concerns, we have updated our Usage Policy to require organizations to follow additional safety measures when using our products in high-risk use cases.\\\\n\\\\n### Expanding who can use our products\\\\n\\\\nWe have also expanded the number of countries whose law enforcement authorities may use our products for a carefully tailored set of use cases, such as call center support and document summarization.\\\\n\\\\n### Clearer privacy protections\\\\n\\\\nOur Acceptable Use Policy has always prohibited gathering information on an individual or group in order to track, target, or report on their identity. We have now updated the policy to explicitly forbid the use of our products to analyze biometric data to infer characteristics like race or religious beliefs. We’ve also added language prohibiting use of our models to build recognition systems or techniques to infer people's emotions for use cases like interrogation.\\\\n\\\\nIn addition, our updated policy makes clear that our products cannot be used to analyze or identify specific content to censor on behalf of a government organization. This applies to all countries where our products are permitted for use.\\\\n\\\\nYou can view our updated policy in full here.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Consumer Terms of Service prohibit the use of our services by individuals under the age of 18. At the same time, there are certain use cases where AI tools can offer significant benefits to younger users, such as test preparation or tutoring support.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3665,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.706831255344761,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Does Anthropic act as a Data Processor or Controller? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/9267385-does-anthropic-act-as-a-data-processor-or-controller?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Does Anthropic Act as a Data Processor or Controller?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n- The customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\nThe customer has control over who can be a member of their team and provides us with instructions for how submitted data can be used.\\\\n\\\\n- The customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\nThe customer may access and export data (such as conversation history) submitted by its Users.\\\\n\\\\n- Anthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\nAnthropic acts as a \\\\\\\"Processor\\\\\\\" of the data on behalf of the customer. Anthropic only processes the data as instructed by the customer in order to provide the Claude service.\\\\n\\\\n- Anthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nAnthropic does not use the data you share when using our commercial products to train our models, unless you choose to participate in our Development Partner Program.\\\\n\\\\nIn summary, for Claude for Work, the customer organization maintains control over their Users' data, and Anthropic only processes that data to provide the service on the customer's behalf, according to our agreement. You should consult with your organization's administrators regarding Claude usage policies.\\\\n\\\\n## Resources\\\\n\\\\nFor more details on our data practices, please see the:\\\\n\\\\n- Anthropic Privacy Center\\\\n\\\\nAnthropic Privacy Center\\\\n\\\\n- Anthropic Trust Center\\\\n\\\\nAnthropic Trust Center\\\",\\n      \\\"description\\\": \\\"When a Commercial Customer creates a Claude for Work account (Team or Enterprise plan), under our Commercial Terms of Service the Customer is the &quot;Controller&quot; of the data submitted by its Users.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1910,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.6582197236529916,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Expanded legal protections and improvements to our API \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/expanded-legal-protections-api-improvements?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Expanded legal protections and improvements to our API\\\\n\\\\n## Messages API beta\\\\n\\\\nIt’s easy to make subtle mistakes when formatting prompts for our existing API — particularly when prompts are dynamically constructed from a mix of user inputs. The new Messages API will help you catch errors early in development, particularly with respect to prompt construction, so that you can get the best output from Anthropic's models.\\\\n\\\\nExample request, before:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/complete\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens_to_sample\\\\\\\": 1024,\\\\n  \\\\\\\"prompt\\\\\\\": \\\\\\\"\\\\\\\\n\\\\\\\\nHuman: Hello, world\\\\\\\\n\\\\\\\\nAssistant: Hi, I'm Claude!\\\\\\\\n\\\\\\\\nHuman: Can you create a template for a quarterly executive brief?\\\\\\\\n\\\\\\\\nAssistant:\\\\\\\"\\\\n}\\\\n```\\\\n\\\\nAfter:\\\\n\\\\n```javascript\\\\n// POST https://api.anthropic.com/v1/messages\\\\n{\\\\n  \\\\\\\"model\\\\\\\": \\\\\\\"claude-2.1\\\\\\\",\\\\n  \\\\\\\"max_tokens\\\\\\\": 1024,\\\\n  \\\\\\\"messages\\\\\\\": [\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hello, world\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"assistant\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Hi, I'm Claude!\\\\\\\" },\\\\n    { \\\\\\\"role\\\\\\\": \\\\\\\"user\\\\\\\", \\\\\\\"content\\\\\\\": \\\\\\\"Can you create a template for a quarterly executive brief?\\\\\\\" }\\\\n  ]\\\\n}\\\\n```\\\\n\\\\nWe have many upcoming features planned that are enabled by a richer, structured API. This beta feature is our first step in offering services like robust function calling, which will be coming to the Messages API soon.In addition to these updates, we plan to broaden access to the Claude API in the coming weeks so developers and enterprises can build with our trusted AI solutions.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Our Commercial Terms of Service (previously our services agreement) will enable our customers to retain ownership rights over any outputs they generate through their use of our services and protect them from copyright infringement claims.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1672,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.5898423685516803,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 15242\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_ee7024ad-52aa-4f43-a0cc-e58ec0e77569\\\",\\n  \\\"query\\\": \\\"site:anthropic.com data protection\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How does Anthropic protect the personal data of Claude users?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\nAt Anthropic, we're committed to protecting your privacy and securing your data. Here's how we keep your information safe:\\\\n\\\\n## Your Data Protection\\\\n\\\\n- Encryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\nEncryption: Your data is automatically encrypted both while in transit, and stored (at rest).\\\\n\\\\n- Limited Access: By default, Anthropic employees cannot access your conversations unless:You explicitly consent to share your data with us as a part of giving us feedback (learn more).Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nLimited Access: By default, Anthropic employees cannot access your conversations unless:\\\\n\\\\n- You explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\nYou explicitly consent to share your data with us as a part of giving us feedback (learn more).\\\\n\\\\n- Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\nReview is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust & Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\\n\\\\n## Our Security Measures\\\\n\\\\nWe use a variety of industry-standard security measures to protect your data, including but not limited to:\\\\n\\\\n## System Security\\\\n\\\\n- Regular security monitoring and vulnerability checks.\\\\n\\\\nRegular security monitoring and vulnerability checks.\\\\n\\\\n- Up-to-date anti-malware protection.\\\\n\\\\nUp-to-date anti-malware protection.\\\\n\\\\n- Secure remote access requiring multi-factor authentication.\\\\n\\\\nSecure remote access requiring multi-factor authentication.\\\\n\\\\n- Mobile device management policies.\\\\n\\\\nMobile device management policies.\\\\n\\\\n- Network segmentation.\\\\n\\\\nNetwork segmentation.\\\\n\\\\n## Organizational Security\\\\n\\\\n- Annual security and privacy training for all employees.\\\\n\\\\nAnnual security and privacy training for all employees.\\\\n\\\\n- Regular security assessments.\\\\n\\\\nRegular security assessments.\\\\n\\\\n- Comprehensive system inventory management.\\\\n\\\\nComprehensive system inventory management.\\\\n\\\\n- Secure company device management policies.\\\\n\\\\nSecure company device management policies.\\\\n\\\\n- Employee access to systems is based on the least privilege principle.\\\\n\\\\nEmployee access to systems is based on the least privilege principle.\\\\n\\\\n## International Data Transfers\\\\n\\\\nAs a global company, we may process data in different countries where we or our partners operate. When transferring data outside the EEA or UK, we ensure protection through:\\\\n\\\\n- Adequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\nAdequacy Decisions: Transfers to countries recognized by the European Commission as having adequate data protection standards.\\\\n\\\\n- Standard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\\n\\\\nStandard Contractual Clauses (SCCs): Legal agreements which include SCCs with our partners to ensure that the appropriate data protection standards are in place.\\\",\\n      \\\"description\\\": \\\"Review is needed to enforce our Usage Policy. In such cases, your conversation data is protected through strict access controls– only designated members of our Trust &amp; Safety team may access this data on a need-to-know basis as a part of their evaluation process.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3799,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.9233803234991451,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"The new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts. To protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Clio: A system for privacy-preserving insights into real-world AI use\\\\n\\\\nAug 28, 2025\\\\n\\\\nWhat do people use AI models for? Despite the rapidly-growing popularity of large language models, until now we’ve had little insight into exactly how they’re being used.\\\\n\\\\nThis isn’t just a matter of curiosity, or even of sociological research. Knowing how people actually use language models is important for safety reasons: providers put considerable effort into pre-deployment testing, and use Trust and Safety systems to prevent abuses. But the sheer scale and diversity of what language models can do makes understanding their uses—not to mention any kind of comprehensive safety monitoring—very difficult.\\\\n\\\\nThere’s also a crucially important factor standing in the way of a clear understanding of AI model use: privacy. At Anthropic, we take the protection of our users’ data very seriously. How, then, can we research and observe how our systems are used while rigorously maintaining user privacy?\\\\n\\\\nClaude insights and observations, or “Clio,” is our attempt to answer this question. Clio is an automated analysis tool that enables privacy-preserving analysis of real-world language model use. It gives us insights into the day-to-day uses of claude.ai in a way that’s analogous to tools like Google Trends. It’s also already helping us improve our safety measures. In this post—which accompanies a full research paper—we describe Clio and some of its initial results.\\\\n\\\\n## How Clio works: Privacy-preserving analysis at scale\\\\n\\\\nTraditional, top-down safety approaches (such as evaluations and red teaming) rely on knowing what to look for in advance. Clio takes a different approach, enabling bottom-up discovery of patterns by distilling conversations into abstracted, understandable topic clusters. It does so while preserving user privacy: data are automatically anonymized and aggregated, and only the higher-level clusters are visible to human analysts.\\\\n\\\\nHere is a brief summary of Clio’s multi-stage process:\\\\n\\\\n- Extracting facets: For each conversation, Clio extracts multiple \\\\\\\"facets\\\\\\\"—specific attributes or metadata such as the conversation topic, number of back-and-forth turns in the conversation, or the language used.\\\\n\\\\n- Semantic clustering: Similar conversations are automatically grouped together by theme or general topic.\\\\n\\\\n- Cluster description: Each cluster receives a descriptive title and summary that captures common themes from the raw data while excluding private information.\\\\n\\\\n- Building hierarchies: Clusters are organized into a multi-level hierarchy for easier exploration. They can then be presented in an interactive interface that analysts at Anthropic can use to explore patterns across different dimensions (topic, language, etc.).\\\\n\\\\nThese four steps are powered entirely by Claude, not by human analysts. This is part of our privacy-first design of Clio, with multiple layers to create “defense in depth.” For example, Claude is instructed to extract relevant information from conversations while omitting private details. We also have a minimum threshold for the number of unique users or conversations, so that low-frequency topics (which might be specific to individuals) aren’t inadvertently exposed. As a final check, Claude verifies that cluster summaries don’t contain any overly specific or identifying information before they’re displayed to the human user.\\\\n\\\\nAll our privacy protections have been extensively tested, as we describe in the research paper.\\\\n\\\\n## How people use Claude: Insights from Clio\\\\n\\\\nUsing Clio, we've been able to glean high-level insights into how people use claude.ai in practice. While public datasets like WildChat and LMSYS-Chat-1M provide useful information on how people use language models, they only capture specific contexts and use cases. Clio allows us to understand the full spectrum of real-world usage of claude.ai (which may look different than usage of other AI systems due to differences in user bases and model types).\\\\n\\\\n### Top use cases on Claude.ai\\\\n\\\\nWe used Clio to analyze 1 million conversations with Claude on claude.ai (both the Free and Pro tiers) to identify the top tasks people use Claude for. This revealed a particular emphasis on coding-related tasks: the \\\\\\\"Web and mobile application development\\\\\\\" category represented over 10% of all conversations. Software developers use Claude for tasks ranging from debugging code to explaining Git operations and concepts.\\\\n\\\\nEducational uses formed another significant category, with more than 7% of conversations focusing on teaching and learning. A substantial percentage of conversations (nearly 6%) concerned business strategy and operations (including tasks like drafting professional communications and analyzing business data).\\\\n\\\\nClio also identified thousands of smaller conversation clusters, showing the rich variety of uses for Claude. Some of these were perhaps surprising, including:\\\\n\\\\n- Dream interpretation;\\\\n\\\\n- Analysis of soccer matches;\\\\n\\\\n- Disaster preparedness;\\\\n\\\\n- “Hints” for crossword puzzles;\\\\n\\\\n- Dungeons & Dragons gaming;\\\\n\\\\n- Counting the r’s in the word “strawberry”.\\\\n\\\\n### Claude usage varies by language\\\\n\\\\nClaude usage varies considerably across languages, reflecting varying cultural contexts and needs. We calculated a base rate of how often each language appeared in conversations overall, and from there we could identify topics where a given language appeared much more frequently than usual. Some examples for Spanish, Chinese, and Japanese are shown in the figure below.\\\\n\\\\n## How we improve our safety systems with Clio\\\\n\\\\nIn addition to training our language models to refuse harmful requests, we also use dedicated Trust and Safety enforcement systems to detect, block, and take action on activity that might violate our Usage Policy. Clio supplements this work to help us understand where there might be opportunities to improve and strengthen these systems.\\\\n\\\\nWe’re still in the process of rolling out Clio across all of our enforcement systems, but so far it has proven to be a useful part of our safety tool kit, helping us discover areas of our protective measures that we need to strengthen.\\\\n\\\\n### Identifying and blocking coordinated misuse\\\\n\\\\nClio has proven effective at identifying patterns of coordinated, sophisticated misuse that would otherwise be invisible when looking at individual conversations, and that might evade simpler detection methods. For example in late September, we identified a network of automated accounts using similar prompt structures to generate spam for search engine optimization. While no individual conversation violated our Usage Policy, the pattern of behavior across accounts revealed a form of coordinated platform abuse we explicitly prohibit in our policy and we removed the network of accounts. We’ve also used Clio to identify other activity prohibited by our Usage Policy, such as attempting to resell unauthorized access to Claude.\\\\n\\\\n### Enhanced monitoring for high-stakes events\\\\n\\\\nClio also helps us monitor novel uses and risks during periods of uncertainty or high-stakes events. For example, while we conducted a wide range of safety tests in advance of launching a new computer use feature, we used Clio to screen for emergent capabilities and harms we might have missed1. Clio provided an additional safeguard here, as well as insights that helped us continually improve our safety measures throughout the rollout and in future versions of our systems.\\\\n\\\\nClio has also helped us monitor for unknown risks in the run up to important public events like elections or major international events. In the months preceding the 2024 US General Election, we used Clio to identify clusters of activity relating to US politics, voting, and related issues, and guard against any potential risks or misuse. The ability to detect “unknown unknowns,” made possible through Clio, complements our proactive safety measures and helps us respond quickly to new challenges.\\\\n\\\\n### Reducing false negatives and false positives\\\\n\\\\nIn general, there was agreement between Clio and our pre-existing Trust and Safety classifiers on which conversation clusters were considered concerning. However, there was some disagreement for some clusters. One opportunity for improvement was false negatives (when a system didn’t flag a particular conversation as potentially harmful when in fact it was). For example, our systems sometimes failed to flag violating content when the user asked Claude to translate from one language to another. Clio, however, spotted these conversations.\\\\n\\\\nWe also used Clio to investigate false positives—another common challenge when developing Trust and Safety classifiers, where the classifier inadvertently tags benign content as harmful. For example, conversations from job seekers requesting advice on their own resumes were sometimes incorrectly flagged by our classifiers (due to the presence of personal information). Programming questions related to security, networking, or web scraping were occasionally misidentified as potential hacking attempts. Even conversations about combat statistics in the aforementioned Dungeons & Dragons conversations sometimes triggered our harm detection systems. We used Clio to highlight these erroneous decisions, helping our safety systems to trigger only for content that really does violate our policies, and otherwise keep out of our users’ way.\\\\n\\\\n## Ethical considerations and mitigations\\\\n\\\\nClio provides valuable insights for improving the safety of deployed language models. However, it did also raise some important ethical considerations that we considered and addressed while developing the system:\\\\n\\\\n- False positives: In the Trust and Safety context, we've implemented key safeguards with respect to potential false positives. For example, at this time we don't use Clio’s outputs for automated enforcement actions, and we extensively validate its performance across different data distributions—including testing across multiple languages, as we detail in our paper.\\\\n\\\\n- Misuse of Clio: A system like Clio could be misused to engage in inappropriate monitoring. In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio.\\\\n\\\\n- User privacy: Despite Clio's strong performance in our privacy evaluations, it's possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected. As time goes on, we also plan to use the latest Claude models in Clio so we can continuously improve the performance of these safeguards.\\\\n\\\\n- User trust: Despite our extensive privacy protections, some users might perceive a system like Clio as invasive or as interfering with their use of Claude. We've chosen to be transparent about Clio's purpose, capabilities, limitations, and what insights we’ve learned from it. And as we noted above, there are instances where Clio identified false positives (where it appeared there was activity violating our usage policy where there wasn’t) in our standard safety classifiers, potentially allowing us to interfere less in legitimate uses of the model.\\\\n\\\\n## Conclusions\\\\n\\\\nClio is an important step toward empirically grounded AI safety and governance. By enabling privacy-preserving analysis of real-world AI usage, we can better understand how these systems are actually used. Ultimately, we can use Clio to make AI systems safer.\\\\n\\\\nAI providers have a dual responsibility: to maintain the safety of their systems while protecting user privacy. Clio demonstrates that these goals aren't mutually exclusive—with careful design and implementation, we can achieve both. By openly discussing Clio, we aim to contribute to positive norms around the responsible development and use of such tools.\\\\n\\\\nWe're continuing to develop and improve Clio, and we hope that others will build upon this work. For additional technical details about Clio, including our privacy validations and evaluation methods, please see the full research paper.\\\\n\\\\nEdit 14 January 2025: Links to the Clio paper in this post have been updated to point to the arXiv version.\\\\n\\\\n#### Footnotes\\\\n\\\\nResearch\\\\n\\\\n### Project Fetch: Can Claude train a robot dog?\\\\n\\\\nNov 12, 2025\\\\n\\\\n### Commitments on model deprecation and preservation\\\\n\\\\nNov 04, 2025\\\\n\\\\n### Signs of introspection in large language models\\\\n\\\\nOct 29, 2025\\\",\\n      \\\"description\\\": \\\"In addition to strict access controls and our privacy techniques, we mitigate this risk by implementing strict data minimization and retention policies: we only collect and retain the minimum amount of data necessary for Clio. User privacy: Despite Clio&#x27;s strong performance in our privacy evaluations, it&#x27;s possible, as with any real-world privacy system, that our systems might not catch certain kinds of private information. To mitigate this potential risk, we regularly conduct audits of our privacy protections and evaluations for Clio to ensure our safeguards are performing as expected.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 12606,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/d9857b663331a721753a3e917dcb985726ebfbf4-2560x1440.png\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F04049f03394efe43a972f52066ecb801fc544a40-2400x1346.png&w=3840&q=75\\\",\\n        \\\"2\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Ff3323b774293075d2e23bb055d024398121e90e7-2400x1066.png&w=3840&q=75\\\",\\n        \\\"3\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2Fd2c1de943736afb84f09bc35561ca8cce183e667-2400x1234.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7432915083467089,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What personal data will be processed by Computer use?\\\\n\\\\nThis article is about our commercial products such as Claude for Work and the Anthropic API. For our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code, see here.\\\\n\\\\n## What is Computer Use?\\\\n\\\\nComputer use is a beta feature included in our new models.\\\\n\\\\nWith computer use, developers can direct Claude to use computers the way people do—by interpreting screen content, moving a cursor, clicking, and inputting text.\\\\n\\\\nComputer use is currently only available to our commercial customers using the Anthropic API, and must be built and enabled by a developer.\\\\n\\\\n## What Personal Data May Be Processed by Computer Use?\\\\n\\\\nWhen prompted by a user, computer use will process and collect screenshots from the computer's display that Claude uses to interpret and interact with the interface, along with the user’s Inputs and Outputs. Anthropic will not collect any additional data from users' computer interface.\\\\n\\\\nBy default, Anthropic will automatically delete all screenshots from our backend within 30 days, unless the customer and Anthropic have agreed to different terms. Anthropic retains this data to help enforce our Usage Policy. Learn more.\\\",\\n      \\\"description\\\": \\\"When prompted by a User, computer use will process and collect screenshots from the computer&#x27;s display that Claude uses to interpret and interact with the interface, along with the user’s Prompts and Outputs. Anthropic will not collect any additional data from users&#x27; computer interface.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1240,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.737115296695831,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers FAQsWhat is your approach to GDPR or related issues?\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)[Consumers](https://privacy.claude.com/en/collections/10663362-consumers)[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)What is your approach to GDPR or related issues?\\\\n\\\\nWhat is your approach to GDPR or related issues?\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nUpdated over 2 weeks ago\\\\n\\\\n# What is your approach to GDPR or related issues?\\\\n\\\\nDid this answer your question?\\\",\\n      \\\"description\\\": \\\"We approach data privacy and security holistically, assessing worldwide privacy laws and regulations together with our customers’ needs in the unique context of artificial intelligence and large language models.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 527,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7222248656761519,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 25257\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_e5d55060-3f5f-437b-a370-013c36b2afa6\\\",\\n  \\\"query\\\": \\\"site:anthropic.com cookie policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Updates to our Acceptable Use Policy (now “Usage Policy”), Consumer Terms of Service, and Privacy Policy\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7238201957602904,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"Starting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.652725111134069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.5465443785834576,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 11771\\n}\"}]}}, {\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_a1d47a8d-91e5-4dec-a4b8-7081e89f7b0a\\\",\\n  \\\"query\\\": \\\"site:anthropic.com privacy policy\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"These updates apply to users on our Claude Free, Pro, and Max plans, including when they use Claude Code from accounts associated with those plans. They do not apply to services under our Commercial Terms, including Claude for Work, Claude for Government, Claude for Education, or API use, including via third parties such as Amazon Bedrock and Google Cloud’s Vertex AI.\\\\n\\\\nBy participating, you’ll help us improve model safety, making our systems for detecting harmful content more accurate and less likely to flag harmless conversations. You’ll also help future Claude models improve at skills like coding, analysis, and reasoning, ultimately leading to better models for all users.\\\\n\\\\nYou’re always in control of this setting and whether we use your data in this way. If you’re a new user, you can select your preference in the signup process. Existing users will see the choice in a pop-up window like the one below.\\\\n\\\\nStarting today, we’re rolling out notifications so you can review these updates and manage your settings. If you’re an existing user, you have until October 8, 2025 to accept the updated Consumer Terms and make your decision. If you choose to accept the new policies now, they will go into effect immediately. These updates will apply only to new or resumed chats and coding sessions. After October 8, you’ll need to make your selection on the model training setting in order to continue using Claude. You can change your choice in your Privacy Settings at any time.\\\\n\\\\n## Extended data retention\\\\n\\\\nWe are also extending data retention to five years, if you allow us to use your data for model training. This updated retention length will only apply to new or resumed chats and coding sessions, and will allow us to better support model development and safety improvements. If you delete a conversation with Claude it will not be used for future model training. If you do not choose to provide your data for model training, you’ll continue with our existing 30-day data retention period.\\\\n\\\\nThe new five-year retention period will also apply to feedback you submit to us about Claude’s responses to prompts.\\\\n\\\\nTo protect users’ privacy, we use a combination of tools and automated processes to filter or obfuscate sensitive data. We do not sell users’ data to third parties.\\\\n\\\\n#### FAQ\\\\n\\\\n#### What’s changing?\\\\n\\\\n- We will train new models using data from Free, Pro, and Max accounts when this setting is on (including when you use Claude Code from these accounts).If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.If you’re a new user, you can pick your setting for model training during the signup process.You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- If you’re a current user, you can select your preference now and your selection will immediately go into effect. This setting will only apply to new or resumed chats and coding sessions on Claude. Previous chats with no additional activity will not be used for model training. You have until October 8, 2025 to make your selection.\\\\n\\\\n- If you’re a new user, you can pick your setting for model training during the signup process.\\\\n\\\\n- You can change your selection at any time in your Privacy Settings.\\\\n\\\\n- We are also expanding our data retention period to five years if you allow us to use your data for model improvement, with this setting only applying to new or resumed chats and coding sessions. If you don't choose this option, you will continue with our existing 30-day data retention period.\\\\n\\\\nThese updates do not apply to services under our Commercial Terms, including:\\\\n\\\\n- Claude for Work, which includes our Team and Enterprise plans\\\\n\\\\n- Our API, Amazon Bedrock, or Google Cloud’s Vertex API\\\\n\\\\n- Claude Gov and Claude for Education\\\\n\\\\n#### Why are you making this change?\\\\n\\\\nAll large language models, like Claude, are trained using large amounts of data. Data from real-world interactions provide valuable insights on which responses are most useful and accurate for users. For example, when a developer debugs code by collaborating with an AI model, that interaction offers valuable signals that help improve future models on similar coding tasks. This creates a feedback loop that helps models get better over time.\\\\n\\\\nIt’s up to you to choose whether to allow your data to be used to improve new Claude models and you can change your choice anytime in your Privacy Settings.\\\\n\\\\n#### Why are you extending the data retention period?\\\\n\\\\nAI development cycles span years—models released today began development 18 to 24 months ago. Keeping data consistent across the training process helps make the models more consistent, too: models trained on similar data will respond, reason, and produce outputs in similar ways, making the changes between model upgrades much smoother for users.\\\\n\\\\nThe extended retention period also helps us improve our classifiers—systems that help us identify misuse—to detect harmful usage patterns. These systems get better at identifying activity like abuse, spam, or misuse when they can learn from data collected over longer periods, helping us keep Claude safe for everyone.\\\\n\\\\nIf you change your setting on providing your data for training or delete your account, we'll exclude your data from future model training. If you delete individual chats, they won't be included in future training either. Learn more about our data retention practices here.\\\\n\\\\n#### What action do I need to take?\\\\n\\\\nCurrent users will see an in-app notification asking whether you want to share your chats and coding sessions for model improvement. You can make your selection right away, or select \\\\\\\"not now\\\\\\\" and decide later. You have until October 8, 2025 to make your choice. If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you'll need to select your preference to continue using Claude.\\\\n\\\\nIf you're signing up for Claude today, you'll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\\n\\\\n#### What happens if I allow my data to be used for model training and then change my mind?\\\\n\\\\nYou can always update your selection in your Privacy Settings. If you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nNews\\\\n\\\\n### Disrupting the first reported AI-orchestrated cyber espionage campaign\\\\n\\\\nNov 13, 2025\\\\n\\\\n### The state of Maryland partners with Anthropic to better serve residents\\\\n\\\\n### Measuring political bias in Claude\\\",\\n      \\\"description\\\": \\\"If you choose this option, the updated 5-year data retention policy will also immediately apply to new and resumed chats and coding sessions. Once October 8 arrives, you&#x27;ll need to select your preference to continue using Claude. If you&#x27;re signing up for Claude today, you&#x27;ll see this decision as part of the signup flow. And remember—you can always update your preference in Privacy Settings.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 7085,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://www.anthropic.com/api/opengraph-illustration?name=Node%20Multi&backgroundColor=sky\\\",\\n        \\\"1\\\": \\\"https://www.anthropic.com/_next/image?url=https%3A%2F%2Fwww-cdn.anthropic.com%2Fimages%2F4zrzovbb%2Fwebsite%2F0d48d7f71fe2b5ccc537b48394313a91636bb329-3840x2160.png&w=3840&q=75\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7934208645064258,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How Do You Use Personal Data in Model Training?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## About Model Training\\\\n\\\\nLarge language models such as Claude are “trained” on a variety of content such as text, images and multimedia so that they can learn the patterns and connections between words and/or content. This training is important so that the model performs effectively and safely.\\\\n\\\\nModels do not store text like a database, nor do they simply “mash-up” or “collage” existing content. Models identify general patterns in text in order to help people create new content, and they do not have access to or pull from the original training data once the models have been trained.\\\\n\\\\n## Collection of Personal Data\\\\n\\\\nThe following sources of training data may contain personal data:\\\\n\\\\n- Publicly available information via the Internet\\\\n\\\\nPublicly available information via the Internet\\\\n\\\\n- Datasets that we obtain under commercial agreements with third party businesses\\\\n\\\\nDatasets that we obtain under commercial agreements with third party businesses\\\\n\\\\n- Data that our users or crowd workers provide\\\\n\\\\nData that our users or crowd workers provide\\\\n\\\\n- Data you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nData you allow us to use, such as your chats and coding sessions, to improve Claude.\\\\n\\\\nWe do not actively set out to use personal data to train our models. However, a large amount of data on the Internet relates to people, so our training data may incidentally include personal data. We only use personal data included in our training data to help our models learn about language and how to understand and respond to it. We do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\n## Privacy Safeguards During Data Collection and Training\\\\n\\\\nWe take steps to minimize the privacy impact on individuals through the training process. We operate under strict policies and guidelines, for instance, that we do not access password protected pages or bypass CAPTCHA controls. We undertake due diligence on the data that we license. And we encourage our users not to use our products and services to process personal data.\\\\n\\\\nWhere you have allowed us to use your chats and coding sessions to improve Claude, we will automatically de-link them from your user ID (e.g. email address) before it’s used by Anthropic.\\\\n\\\\nWe will use tools and processes derived from our work on privacy-preserving analysis tools to filter or obfuscate sensitive data. In addition to that, our pioneering research in post-training techniques helps to minimize the amount of personal data included in Claude’s outputs.\\\\n\\\\nYour data is used solely to make Claude better for everyone - we do not use such personal data to contact people, build profiles about them, to try to sell or market anything to them, or to sell the information itself to any third party.\\\\n\\\\nAdditionally, you maintain full control and can adjust your privacy settings at any time.\\\\n\\\\n## Data usage for Claude.ai Consumer Offerings (e.g. Claude, Pro, Max, etc.)\\\\n\\\\nWe may use your chats or coding sessions to improve our models, if:\\\\n\\\\n- You choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\nYou choose to allow us to use your chats and coding sessions to improve Claude,\\\\n\\\\n- Your conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\nYour conversations are flagged for safety review (in which case we may use or analyze them to improve our ability to detect and enforce our Usage Policy, including training models for use by our Safeguards team, consistent with Anthropic’s safety mission),\\\\n\\\\n- You’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\nYou’ve explicitly provided materials to us (e.g.via our thumbs up/down feedback button), or\\\\n\\\\n- By otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\nBy otherwise explicitly opting in to training (e.g. by joining our Trusted Tester Program).\\\\n\\\\n## Privacy Rights and Data Processing\\\",\\n      \\\"description\\\": \\\"Our Privacy Policy explains your rights regarding your personal data, including with respect to our training activities. This includes your right to request a copy of your personal data, and to object to our processing of your personal data or request that it is deleted.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 4471,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7915228444900766,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# How long do you store my data?\\\\n\\\\nThis article is about our consumer products such as Claude Free, Pro, Max and when accounts from those plans use Claude Code. For our commercial products such as Claude for Work and the Anthropic API, see here.\\\\n\\\\n## Standard Retention Timeframe\\\\n\\\\n## If You Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you allow us to use your chats or coding sessions to improve Claude, we will retain your data for up to 5 years. This retention period only applies to new or resumed chats, once data sharing is enabled. You can learn more about how our models are trained here.\\\\n\\\\nIf you choose to provide your data for model training, you can change your privacy settings at any time following these steps here.\\\\n\\\\nIf you decide to turn off the model training setting, we will not use any new chats and coding sessions you have with Claude for future model training. Your data will still be included in model training that has already started and in models that have already been trained, but we will stop using your previously stored chats and coding sessions in future model training runs.\\\\n\\\\nYour Incognito chats are not used to improve Claude, even if you have enabled Model Improvement in your Privacy Settings. Learn more about Incognito chats.\\\\n\\\\n## If You Do Not Allow Us to Use Your Data to Improve Claude\\\\n\\\\nIf you choose not to allow us to use your chats and coding sessions to improve Claude, your chats will be retained in our back-end storage systems for up to 30 days.\\\\n\\\\n## Controlling Your Chat History\\\\n\\\\nYou control your chat history and can delete your conversations at any time. When you delete a conversation it’s:\\\\n\\\\n- Removed from your chat history immediately\\\\n\\\\nRemoved from your chat history immediately\\\\n\\\\n- Deleted from our back-end storage systems within 30 days ​\\\\n\\\\nDeleted from our back-end storage systems within 30 days ​\\\\n\\\\n## Usage Policy Violations\\\\n\\\\nWe retain inputs and outputs for up to 2 years and trust and safety classification scores for up to 7 years if your chat or session is flagged by our trust and safety classifiers as violating our Usage Policy.\\\\n\\\\nIn all cases, we may retain chats and coding sessions as required by law or as necessary to combat violations of our Usage Policy.\\\\n\\\\n## Feedback Data\\\\n\\\\nWhere you have provided feedback to us (e.g. by submitting feedback through our thumbs up/down button or sent bug reports), we retain data associated with that submission for 5 years. Learn more.\\\\n\\\\n## Additional Information\\\\n\\\\nWe may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer.\\\",\\n      \\\"description\\\": \\\"Where you have provided feedback ... with that submission for 5 years. Learn more. We may anonymize or de-identify your personal data for research or statistical purposes, in which case we may retain this information for longer. For more information about our privacy practices, ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2615,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.7783632302649002,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s Transparency Hub \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/transparency/platform-security?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s Transparency Hub: System Trust and Reporting\\\\n\\\\nA look at Anthropic's key processes, programs, and practices for responsible AI development.\\\\n\\\\n01Model Report\\\\n\\\\n02System Trust and Reporting\\\\n\\\\n03Voluntary Commitments\\\\n\\\\n## System Trust and Reporting\\\\n\\\\nLast updated August 7, 2025\\\\n\\\\nWe are sharing more detail on our Usage Policy, some enforcement data, and how we handle legal requests to enable meaningful public dialogue about AI platform safety.\\\\n\\\\n### Banned Accounts\\\\n\\\\n- 690kBanned AccountsJanuary - June 2025\\\\n\\\\n690k\\\\n\\\\nBanned Accounts\\\\n\\\\nJanuary - June 2025\\\\n\\\\nAnthropic’s Safeguards Team designs and implements detections and monitoring to enforce our Usage Policy. If we learn that a user has violated our Usage Policy, we may take enforcement actions such as warning, suspending, or terminating their access to our products and services.\\\\n\\\\n- 35kAppealsJanuary - June 2025\\\\n\\\\n35k\\\\n\\\\nAppeals\\\\n\\\\n- 1.0kAppeal OverturnsJanuary - June 2025\\\\n\\\\n1.0k\\\\n\\\\nAppeal Overturns\\\\n\\\\nBanned users may file appeals to request a review of our decision to ban their account.\\\\n\\\\nJuly - December 2024 Reporting\\\\n\\\\n### Child Safety Reporting\\\\n\\\\n- 613Total pieces of content reported to NCMECJanuary - June 2025\\\\n\\\\n613\\\\n\\\\nTotal pieces of content reported to NCMEC\\\\n\\\\nAnthropic is committed to combating child exploitation through prevention, detection and reporting. On our first-party services, we employ hash-matching technology to detect and report known CSAM to NCMEC that users may upload.\\\\n\\\\n### Legal Requests\\\\n\\\\nAnthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy. These requests may include content information, non-content records, or emergency disclosure requests.\\\\n\\\\nJanuary - June 2024 Government Requests for Data\\\\n\\\\nJuly - December 2024 Government Requests for Data\\\",\\n      \\\"description\\\": \\\"Anthropic processes data requests from law enforcement agencies and governments in accordance with applicable laws while protecting user privacy.\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 1817,\\n      \\\"image_url\\\": {},\\n      \\\"relevance_score\\\": 0.669348098301069,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Anthropic’s AI for Science Program \\\\\\\\ Anthropic\\\",\\n      \\\"url\\\": \\\"https://www.anthropic.com/ai-for-science-program-rules?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# Anthropic’s AI for Science Program Official Rules\\\\n\\\\nLast updated Jun 3, 2025\\\\n\\\\n- Binding Decisions. By applying to and/or participating in the Program, you agree to be bound by the decisions of Anthropic. Anthropic will choose the winning researcher in its sole discretion, and the API Credits (as defined below) will be awarded in accordance with these Rules.\\\\n\\\\n- Eligibility Criteria. If you are an individual, then you must be at least 18 years of age or, if greater, the age of majority in your jurisdiction of residence at the time of application. If you are employed by a legal entity or affiliated with a university or other research institution, then you represent and warrant that you have obtained all consents from that employer or institution necessary to participate in the Program. You may not be a legal resident of Belarus, China, Cuba, Iran, Myanmar, North Korea, Russia, Sudan, Syria, Crimea, and the so-called Donetsk People’s Republic or Luhansk People’s Republic. You may not be a person prohibited from participating in the Program or receiving funds or services under U.S. or other applicable laws, regulations, or export controls. Employees, officers, directors, agents, representatives and their immediate families (spouse, parents, children, siblings and each of their spouses regardless of where they live) or members of household of Anthropic and anyone connected with the operation of this Program are not eligible to enter or be awarded API Credits.\\\\n\\\\n- Applications.All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- All applications are made by, and are the responsibility of, the applicant. Anthropic will correspond with and, if applicable, award API Credits to the applicant. If the applicant is a team, then Anthropic will, if applicable, award the API Credits to the team. In the event of any dispute as to the identity of an applicant, Anthropic will determine the identity of the applicant in its sole discretion. Any dispute among team members must be resolved by and among team members. A dispute among team members may result in disqualification.\\\\n\\\\n- You must have all necessary rights to submit your application to the Program. You represent and warrant that your application contains only your original work and is not in violation of any law, regulation, or third-party rights.\\\\n\\\\n- Only fully complete and compliant applications are eligible to receive API Credits pursuant to these Rules, with such eligibility determined in Anthropic’s sole discretion. All interpretations of these Rules and decisions made by Anthropic relating to the Program are final and binding in all respects.\\\\n\\\\n- You acknowledge that Anthropic may have developed or commissioned materials similar or identical to your application, and that other applicants may provide materials that are similar to the materials in your application, and you waive any claims resulting from such similarities.\\\\n\\\\n- Judging Criteria and Researcher Selection. Entries that fail to meet the eligibility criteria under these Rules, including the application requirements specified above, are deemed incomplete and, at the discretion of Anthropic, may be disqualified.On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- On the first Monday of each month, Anthropic will select winning applications from among all eligible entries received as part of the Program (each applicant that submits a winning application, a “Selected Researcher”). Each Selected Researcher will be selected based on an objective evaluation of their application responses.\\\\n\\\\n- Applications will also undergo a biosecurity assessment to ensure they do not raise concerns around accelerating harmful applications. Anthropic will select Selected Researchers based on these criteria, with the highest-scoring eligible applications receiving API Credits. All judging decisions are final and binding. Anthropic reserves the right to disqualify any application at its sole discretion if it is submitted in bad faith or raises any concerns regarding compliance with these Rules.\\\\n\\\\n- Selected Researcher Notification. Becoming a Selected Researcher is subject to validation and verification of eligibility and compliance with all the Rules. The potential Selected Researchers will be selected and notified via the email address provided at the time of registration. If a potential Selected Researcher does not respond to the notification attempt within five (5) business days after the first notification attempt, then the potential Selected Researcher may be disqualified and an alternate potential Selected Researcher may be selected from among qualifying entries based on the judging criteria described in these Rules. The potential Selected Researcher (and, if the potential Selected Researcher is a team or entity, each natural person associated with the potential Selected Researcher) may be required to sign an affidavit of certifying compliance with these Rules along with a publicity and liability release, each of which, if requested, must be completed, signed, and returned to Anthropic within fourteen calendar (14) days from the date of Anthropic’s request, or the API Credits may be forfeited and awarded to an alternate applicant. Anthropic is not responsible for any change of an applicant’s email address, mailing address, or telephone number.\\\\n\\\\n- Prize and Related Terms. Selected Researchers will receive Anthropic API credits (the “API Credits”). API Credits will be placed in the account for the organization identified in your application.\\\\n\\\\n- Publicity. Except where prohibited by law, you, on behalf of yourself and the entity or team you are associated with (including all team members), grant permission to Anthropic to use or publish your names, biographical information, photographs, voices and/or likenesses for advertising and promotional purposes worldwide in perpetuity without compensation or notification to or permission of any kind, except as prohibited by law.\\\\n\\\\n- Taxes. API Credits awarded under this Program may have different tax implications than cash prizes. Selected Researchers are solely responsible for determining whether receipt of API Credits creates any tax obligations in their jurisdiction. By accepting API Credits, Selected Researchers acknowledge that (a) Anthropic makes no representations regarding the tax treatment of API Credits; (b) Anthropic will not issue tax documentation (such as 1099 forms or equivalents) for API Credits unless specifically required by applicable law; and (c) they are encouraged to consult with their own tax advisors regarding any potential tax implications. If, under applicable law, Anthropic is required to collect any tax information or documentation related to the award of API Credits, Selected Researchers agree to provide such information upon request. If a potential Selected Researcher fails to provide such documentation or comply with such laws, the API Credits may be forfeited and Anthropic may, in its sole discretion, select an alternative potential Selected Researcher.\\\\n\\\\n- Cancellation, Suspension or Amendment. Anthropic reserves the right in its sole discretion to cancel, terminate, modify, or suspend the Program. Anthropic reserves the right to correct any typographical, printing, computer programming or operator errors, including without limitation computer errors that erroneously award API Credits. You hereby acknowledge and agree that you may not assert any claims, demands, or actions of any kind arising from or in connection with the cancellation, suspension or amendment of the Program.\\\\n\\\\n- Disclaimer. Anthropic disclaims all warranties not expressly stated in these Rules. Anthropic does not guarantee uninterrupted service, specific outcomes from use of the API Credits, or that the API Credits will meet Selected Researchers’ specific requirements or expectations. The API Credits are provided “as is”.\\\\n\\\\n- Limitation of Liability. TO THE FULLEST EXTENT PERMITTED BY LAW, ANTHROPIC WILL NOT BE LIABLE FOR ANY INDIRECT, INCIDENTAL, SPECIAL, CONSEQUENTIAL, OR PUNITIVE DAMAGES ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM. ANTHROPIC'S TOTAL LIABILITY TO YOU FOR ALL CLAIMS ARISING FROM OR RELATED TO THESE RULES OR THE PROGRAM, WHETHER IN CONTRACT, TORT, OR OTHERWISE, IS LIMITED TO THE ACTUAL VALUE OF API CREDITS AWARDED TO YOU OR $1,000, WHICHEVER IS LESS. THIS LIMITATION DOES NOT APPLY TO CLAIMS THAT CANNOT BE LIMITED UNDER APPLICABLE LAW.\\\\n\\\\n- Governing Law and Disputes. These Rules and the Program shall be governed by the laws of the State of California, excluding all conflict of law rules. The exclusive forum and venue for any dispute arising from these Rules or the Program shall exclusively be in San Francisco, California. To the extent permitted by law, you waive your rights to seek injunctive relief.\\\\n\\\\n- Miscellaneous. These Rules are Anthropic’s and your entire agreement regarding the subject matter herein and supersede any prior or contemporaneous agreements regarding such subject matter. These Rules are entered into solely between, and may be enforced only by, Anthropic and you. These Rules will not be deemed to create any rights in third parties or to create any obligations of a party to any such third parties. In these Rules, headings are for convenience only and “including” and similar terms are to be construed without limitation. If any provision of these Rules is held to be invalid or unenforceable, it will be limited to the minimum extent necessary so that the rest of these Rules remain in full force and effect. Waivers must be signed by the waiving party’s authorized representative and cannot be implied from conduct. Except as otherwise expressly set forth in these Rules, any amendments, modifications, or supplements to these Rules must be in writing and signed by each party’s authorized representatives or, as appropriate, agreed through electronic means provided by Anthropic.\\\",\\n      \\\"description\\\": \\\"Anthropic’s AI for Science Program (the “Program”) is sponsored by Anthropic, PBC (“Anthropic,” “we,” “our,” or “us”). The Program and all entries are subject to and governed by Anthropic’s privacy policy available at https://www.anthropic.com/legal/privacy (“Privacy ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 11980,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://cdn.sanity.io/images/4zrzovbb/website/c07f638082c569e8ce1e89ae95ee6f332a98ec08-2400x1260.jpg\\\"\\n      },\\n      \\\"relevance_score\\\": 0.616630745890077,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 5,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 7.5,\\n  \\\"total_deduction_dollars\\\": 0.0075,\\n  \\\"total_characters\\\": 27968\\n}\"}]}}]}, {\"role\": \"tool\", \"content\": [{\"toolResult\": {\"toolUseId\": \"valyu_search\", \"status\": \"success\", \"content\": [{\"text\": \"{\\n  \\\"success\\\": true,\\n  \\\"error\\\": \\\"\\\",\\n  \\\"tx_id\\\": \\\"tx_edc96ff4-ee89-49d8-9f9b-05b8f2a3ef70\\\",\\n  \\\"query\\\": \\\"site:anthropic.com What Cookies Does Anthropic Use?\\\",\\n  \\\"results\\\": [\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Help Center\\\",\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Analytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.97,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023541-what-cookies-does-anthropic-use?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"# What Cookies Does Anthropic Use?\\\\n\\\\nThe following tables provide brief descriptions of the cookies used on Anthropic websites.\\\\n\\\\n## Necessary Cookies\\\\n\\\\nNecessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security.\\\\n\\\\nCookie Name\\\\n\\\\nPurpose\\\\n\\\\nDomain\\\\n\\\\nFirst or Third Party\\\\n\\\\nLifespan\\\\n\\\\nsessionKey\\\\n\\\\nAuthentication\\\\n\\\\n.anthropic.com, .claude.ai, console.anthropic.com\\\\n\\\\nFirst\\\\n\\\\n1 month\\\\n\\\\nactivitySessionId\\\\n\\\\n.anthropic.com, claude.ai, .console.anthropic.com\\\\n\\\\n12 hours\\\\n\\\\nCH-prefers-color-scheme\\\\n\\\\nPreferences\\\\n\\\\n1 year\\\\n\\\\nlastActiveOrg\\\\n\\\\n.anthropic.com, claude.ai,\\\\n\\\\n__ssid\\\\n\\\\nSecurity\\\\n\\\\n.anthropic.com, .claude.ai\\\\n\\\\n13 months\\\\n\\\\nanthropic-device-id\\\\n\\\\nclaude.ai,\\\\n\\\\n.console.anthropic.com\\\\n\\\\n10 months\\\\n\\\\nanthropic-consent-preferences\\\\n\\\\nconsole-sidebar-expanded\\\\n\\\\nuser-sidebar-visible-on-load\\\\n\\\\n.claude.ai\\\\n\\\\n__stripe_mid\\\\n\\\\nStripe\\\\n\\\\n__cf_bm\\\\n\\\\nCloudflare\\\\n\\\\n30 minutes\\\\n\\\\ncf_clearance\\\\n\\\\nGoogle security cookies (various)\\\\n\\\\nFunctionality and Security\\\\n\\\\n.google.com\\\\n\\\\nGoogle\\\\n\\\\nvarious\\\\n\\\\nintercom-device-id-#######\\\\n\\\\nIntercom\\\\n\\\\n9 months\\\\n\\\\nintercom-id-#######\\\\n\\\\nSupport Preferences\\\\n\\\\n1 week\\\\n\\\\nintercom-session-#######\\\\n\\\\n## Analytics Cookies\\\\n\\\\nAnalytics cookies are not necessary to deliver our Services, but they do enhance performance of our Services - for example: analyzing website traffic or performance, or understanding how you interact with our website or Services. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\najs_anonymous_id, ajs_user_id\\\\n\\\\nPerformance and Analytics\\\\n\\\\n.anthropic.com, claude.ai\\\\n\\\\nGoogle Analytics cookies (various)\\\\n\\\\nli_giant\\\\n\\\\nConversion tracking analytics\\\\n\\\\nLinkedIn\\\\n\\\\n7 days\\\\n\\\\noribi_cookie_test\\\\n\\\\nSession\\\\n\\\\noribili_user_guid\\\\n\\\\nAnalytics\\\\n\\\\nln_or\\\\n\\\\n1 day\\\\n\\\\n## Marketing Cookies\\\\n\\\\nMarketing cookies are not necessary to deliver our Services, but they do help us market our Services more effectively to users that we, our service providers, and our business partners believe may be interested in using our Services. They also help provide us with aggregated reporting and help us know when marketing content has been shown to you. You can control how and when your personal data is shared or used by clicking here or enabling global privacy controls.\\\\n\\\\nGoogle Advertising cookies (various)\\\\n\\\\nTargeted Marketing\\\\n\\\\n_fbc\\\\n\\\\nFacebook\\\\n\\\\n2 years\\\\n\\\\n_fbp\\\\n\\\\n90 days\\\\n\\\\n_rdt_uuid\\\\n\\\\nReddit\\\\n\\\\n_rdt_cid\\\\n\\\\n_ttclid\\\\n\\\\nTikTok\\\\n\\\\nguest_id\\\\n\\\\n.twitter.com\\\\n\\\\nTwitter\\\\n\\\\n348 days\\\\n\\\\nguest_id_ads\\\\n\\\\nguest_id_marketing\\\\n\\\\npersonalization_id\\\\n\\\\nmuc\\\\n\\\\n.t.co\\\\n\\\\n400 days\\\\n\\\\nmuc_ads\\\\n\\\\n## Cookie Settings\\\\n\\\\nYou can control how and when your personal data is shared or used in the following ways:\\\\n\\\\n- Clicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\nClicking on \\\\\\\"Privacy Choices\\\\\\\" in the footer of our main website (anthropic.com)\\\\n\\\\n- Clicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\nClicking on \\\\\\\"Your privacy choices\\\\\\\" in the Learn More overflow menu on claude.ai:\\\\n\\\\n- Enabling global privacy controls in your browser\\\\n\\\\nEnabling global privacy controls in your browser\\\\n\\\\n​\\\",\\n      \\\"description\\\": \\\"Necessary cookies are used to provide basic functionality of our Services and cannot be refused - for example: authentication, site preferences (including cookie opt-out preference), or security. Analytics cookies are not necessary to deliver our Services, but they do enhance performance of ...\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 3115,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/1817494329/163e6f74386ccc16f2faf048529c/image.png?expires=1763273700&signature=e3f0f73cca2a4d0e410815cd1192b72cc61acc563edba57ed66a5894ae3dc665&req=dSgmEc13mYJdUPMW1HO4zXrC0irziLNzokxDdhQCTHqolwMbF4Deo7FFKAGj%0AtMqa%0A\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.968687908314245,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    },\\n    {\\n      \\\"title\\\": \\\"Consumers | Anthropic Privacy Center\\\",\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/collections/10663362-consumers?utm_source=valyu.ai&utm_medium=referral\\\",\\n      \\\"content\\\": \\\"All CollectionsConsumers\\\\n\\\\n[All Collections](https://privacy.anthropic.com/en/)Consumers\\\\n\\\\nConsumers\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\n22 articles\\\\n\\\\n# Consumers\\\\n\\\\nClaude Free, Pro & Max plans\\\\n\\\\nCookies[What Cookies Does Anthropic Use?](https://privacy.claude.com/en/articles/10023541-what-cookies-does-anthropic-use)\\\\n\\\\nData Handling & Retention[How long do you store my data?](https://privacy.claude.com/en/articles/10023548-how-long-do-you-store-my-data)[How Do You Use Personal Data in Model Training?](https://privacy.claude.com/en/articles/10023555-how-do-you-use-personal-data-in-model-training)[Is my data used for model training?](https://privacy.claude.com/en/articles/10023580-is-my-data-used-for-model-training)[What personal data is collected when using dictation on the Claude mobile apps?](https://privacy.claude.com/en/articles/10067979-what-personal-data-is-collected-when-using-dictation-on-the-claude-mobile-apps)[How does Anthropic protect the personal data of Claude users?](https://privacy.claude.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-users)[How does Clio analyze usage patterns while protecting user data?](https://privacy.claude.com/en/articles/10807912-how-does-clio-analyze-usage-patterns-while-protecting-user-data)[Does Claude use my location?](https://privacy.claude.com/en/articles/11186740-does-claude-use-my-location)\\\\n\\\\n[Data Handling & Retention](https://privacy.claude.com/en/collections/10672565-data-handling-retention)\\\\n\\\\nData Handling & Retention\\\\n\\\\nPrivacy Settings & Controls[Can you delete data sent via Claude?](https://privacy.claude.com/en/articles/7996878-can-you-delete-data-sent-via-claude)[What is Anthropic’s policy for handling governmental requests for user information?](https://privacy.claude.com/en/articles/10023650-what-is-anthropic-s-policy-for-handling-governmental-requests-for-user-information)[Deleting Claude accounts](https://privacy.claude.com/en/articles/10023660-deleting-claude-accounts)[How can I export my Claude data?](https://privacy.claude.com/en/articles/9450526-how-can-i-export-my-claude-data)[Sharing and Unsharing Chats](https://privacy.claude.com/en/articles/10593882-sharing-and-unsharing-chats)[How can I delete or rename a conversation?](https://privacy.claude.com/en/articles/10023670-how-can-i-delete-or-rename-a-conversation)[How do I change my model improvement privacy settings?](https://privacy.claude.com/en/articles/12109829-how-do-i-change-my-model-improvement-privacy-settings)\\\\n\\\\n[Privacy Settings & Controls](https://privacy.claude.com/en/collections/10672568-privacy-settings-controls)\\\\n\\\\nPrivacy Settings & Controls\\\\n\\\\nFAQs[What is your approach to GDPR or related issues?](https://privacy.claude.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues)\\\\n\\\\n[FAQs](https://privacy.claude.com/en/collections/10672566-faqs)\\\\n\\\\nFAQs\\\",\\n      \\\"description\\\": \\\"Claude.ai &amp; Claude.ai Pro users · 22 articles · Cookies · What Cookies Does Anthropic Use? Data Handling &amp; Retention · How long do you store my data? How Do You Use Personal Data in Model Training? Is my data used for model training? What personal data is collected when using dictation on the Claude mobile apps?\\\",\\n      \\\"source\\\": \\\"web\\\",\\n      \\\"price\\\": 0.0015,\\n      \\\"length\\\": 2869,\\n      \\\"image_url\\\": {\\n        \\\"0\\\": \\\"https://intercom.help/privacy-center/assets/svg/icon:people-chat-gets-users/F0F0EB\\\",\\n        \\\"1\\\": \\\"https://downloads.intercomcdn.com/i/o/lupk8zyo/589147/0f6a8b2e289e487afce6e2749695/31367edc0f1b659767b4d77403818b20.png\\\"\\n      },\\n      \\\"relevance_score\\\": 0.798185850487796,\\n      \\\"data_type\\\": \\\"unstructured\\\"\\n    }\\n  ],\\n  \\\"results_by_source\\\": {\\n    \\\"web\\\": 3,\\n    \\\"proprietary\\\": 0\\n  },\\n  \\\"total_deduction_pcm\\\": 4.5,\\n  \\\"total_deduction_dollars\\\": 0.0045,\\n  \\\"total_characters\\\": 9099\\n}\"}]}}]}]", "mlflow.spanOutputs": "[{\"text\": \"```json\\n{\\n  \\\"status\\\": \\\"success\\\",\\n  \\\"company_or_url\\\": \\\"Anthropic\\\",\\n  \\\"resolved_domain\\\": \\\"anthropic.com\\\",\\n  \\\"sources\\\": [\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9020432-what-cookies-does-anthropic-use\\\",\\n      \\\"policy_type\\\": \\\"cookie_policy\\\",\\n      \\\"title\\\": \\\"What Cookies Does Anthropic Use? | Anthropic Help Center\\\",\\n      \\\"summary\\\": \\\"Anthropic uses necessary cookies for basic service functionality (authentication, preferences, security), analytics cookies to enhance performance and understand user interaction, and marketing cookies for targeted advertising. Users can control cookie settings via 'Privacy Choices' on their website footer, 'Your privacy choices' in the Claude.ai menu, or by enabling global privacy controls in their browser.\\\",\\n      \\\"relevance\\\": 0.97,\\n      \\\"relevance_explanation\\\": \\\"Directly addresses the cookie policy and provides detailed information on types of cookies and user controls.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10458704-how-does-anthropic-protect-the-personal-data-of-claude-ai-users\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"How does Anthropic protect the personal data of Claude.ai users? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"Anthropic protects user data through encryption (in transit and at rest) and limited employee access, with strict controls for Trust & Safety reviews. They implement industry-standard security measures including regular monitoring, anti-malware, multi-factor authentication, and annual employee training. For international data transfers, Anthropic ensures protection via adequacy decisions or Standard Contractual Clauses (SCCs).\\\",\\n      \\\"relevance\\\": 0.92,\\n      \\\"relevance_explanation\\\": \\\"Provides a comprehensive overview of data protection measures for Claude.ai users, including security, access, and international transfers.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updates-to-our-consumer-terms\\\",\\n      \\\"policy_type\\\": \\\"privacy_policy\\\",\\n      \\\"title\\\": \\\"Updates to Consumer Terms and Privacy Policy \\\\\\\\\\\\\\\\ Anthropic\\\",\\n      \\\"summary\\\": \\\"Anthropic updated its Consumer Terms and Privacy Policy, allowing user data from Free, Pro, and Max plans (including Claude Code) to be used for model training, with user consent. Data retention is extended to five years if users opt-in for model training; otherwise, it remains 30 days. Anthropic uses tools to filter sensitive data and does not sell user data to third parties. Users can manage their preferences in Privacy Settings.\\\",\\n      \\\"relevance\\\": 0.79,\\n      \\\"relevance_explanation\\\": \\\"Details significant updates to privacy policy, specifically regarding data usage for model training, retention periods, and user control.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996885-how-do-you-use-personal-data-in-model-training\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"How do you use personal data in model training? | Anthropic Help Center\\\",\\n      \\\"summary\\\": \\\"Anthropic trains its large language models using publicly available information, commercially licensed datasets, and user-provided data (with consent). While personal data may be incidentally included, it is not actively sought for training and is used solely to improve model effectiveness and safety, not for profiling or marketing. Privacy safeguards include de-linking user IDs from chats, filtering sensitive data, and post-training techniques to minimize personal data in outputs. Users retain control over their privacy settings.\\\",\\n      \\\"relevance\\\": 0.79,\\n      \\\"relevance_explanation\\\": \\\"Explains the sources of data for model training, how personal data is handled during training, and the privacy safeguards in place.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/7996866-how-long-do-you-store-personal-data\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"How long do you store my data? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"Anthropic retains user data for up to 5 years if users consent to its use for model improvement, applying only to new or resumed chats. If consent is not given, chats are retained for 30 days. Users can delete conversations, which are removed from history immediately and from backend systems within 30 days. Data flagged for Usage Policy violations may be retained for up to 2 years (inputs/outputs) or 7 years (classification scores). Feedback data is retained for 5 years, and anonymized data may be kept longer for research.\\\",\\n      \\\"relevance\\\": 0.78,\\n      \\\"relevance_explanation\\\": \\\"Provides clear details on data retention periods based on user consent for model training and other scenarios like deletions and policy violations.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://www.anthropic.com/research/clio\\\",\\n      \\\"policy_type\\\": \\\"other\\\",\\n      \\\"title\\\": \\\"Clio: Privacy-preserving insights into real-world AI use \\\\\\\\\\\\\\\\ Anthropic\\\",\\n      \\\"summary\\\": \\\"Anthropic developed 'Clio,' an automated analysis tool for privacy-preserving insights into real-world AI usage. Clio distills conversations into anonymized and aggregated topic clusters, allowing Anthropic to understand how models are used without exposing private user data. This system helps improve safety measures by identifying patterns of misuse and enhancing monitoring for high-stakes events, while rigorously maintaining user privacy through multi-layered safeguards and data minimization.\\\",\\n      \\\"relevance\\\": 0.74,\\n      \\\"relevance_explanation\\\": \\\"Describes a specific system (Clio) designed for privacy-preserving data analysis, offering insights into how Anthropic approaches data usage for safety and improvement while protecting user privacy.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10030352-what-personal-data-will-be-processed-by-computer-use-beta\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"What personal data will be processed by computer use (beta)? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"For commercial customers using the Anthropic API, the 'Computer use' beta feature processes and collects screenshots from the computer's display, along with user inputs and outputs, to enable Claude to interpret and interact with interfaces. Anthropic does not collect additional data from the computer interface. Screenshots are automatically deleted from the backend within 30 days, unless otherwise agreed, and are retained to enforce the Usage Policy.\\\",\\n      \\\"relevance\\\": 0.74,\\n      \\\"relevance_explanation\\\": \\\"Details the specific personal data collected and retained for a particular beta feature ('Computer use') and its purpose.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://support.anthropic.com/en/articles/9190861-terms-of-service-updates\\\",\\n      \\\"policy_type\\\": \\\"terms_of_service\\\",\\n      \\\"title\\\": \\\"Terms of Service Updates | Anthropic Help Center\\\",\\n      \\\"summary\\\": \\\"Anthropic updated its Terms of Service to clarify the definition of 'Providers' to include affiliates, licensors, distributors, and service providers, making them intended third-party beneficiaries of disclaimers and liability limitations. The updates also clarify terms regarding software updates, including automatic updates to ensure users have access to the latest versions of their software.\\\",\\n      \\\"relevance\\\": 0.72,\\n      \\\"relevance_explanation\\\": \\\"Provides specific updates to the Terms of Service, focusing on definitions and software updates.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://privacy.anthropic.com/en/articles/10023628-what-is-your-approach-to-gdpr-or-related-issues\\\",\\n      \\\"policy_type\\\": \\\"data_protection\\\",\\n      \\\"title\\\": \\\"What is your approach to GDPR or related issues? | Anthropic Privacy Center\\\",\\n      \\\"summary\\\": \\\"Anthropic adopts a holistic approach to data privacy and security, evaluating worldwide privacy laws and regulations, including GDPR, in the context of AI and large language models. They assess these regulations alongside customer needs to ensure comprehensive data protection practices.\\\",\\n      \\\"relevance\\\": 0.72,\\n      \\\"relevance_explanation\\\": \\\"Explains Anthropic's general approach to GDPR and global privacy regulations.\\\"\\n    },\\n    {\\n      \\\"url\\\": \\\"https://www.anthropic.com/news/updating-our-usage-policy\\\",\\n      \\\"policy_type\\\": \\\"terms_of_service\\\",\\n      \\\"title\\\": \\\"Updating our Usage Policy \\\\\\\\\\\\\\\\ Anthropic\\\",\\n      \\\"summary\\\": \\\"Anthropic updated its Acceptable Use Policy, now called 'Usage Policy,' to clarify allowed and prohibited applications of its products. Key changes include streamlining policies into 'Universal Usage Standards,' clarifying rules on election integrity and misinformation, adding requirements for high-risk use cases (e.g., healthcare, legal guidance), expanding law enforcement use cases, and enhancing privacy protections by explicitly forbidding biometric data analysis for inferring characteristics or content censorship on behalf of governments.\\\",\\n      \\\"relevance\\\": 0.71,\\n      \\\"relevance_explanation\\\": \\\"Details updates to their Usage Policy, which outlines acceptable use of their products and includes important privacy and data-related stipulations.\\\"\\n    }\\n  ],\\n  \\\"error_message\\\": null\\n}\\n```\"}]", "mlflow.chat.tokenUsage": "{\"input_tokens\": 44070, \"output_tokens\": 4932, \"total_tokens\": 49002}"}}]}